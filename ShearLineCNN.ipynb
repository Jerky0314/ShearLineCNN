{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae5d8f43-6828-4068-be52-b1695b6f0c43",
   "metadata": {
    "id": "ae5d8f43-6828-4068-be52-b1695b6f0c43"
   },
   "source": [
    "# ShearLineCNN\n",
    "> Shear Line Classification using CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db17189e-3c33-4faa-8b17-2085be1342a5",
   "metadata": {
    "id": "db17189e-3c33-4faa-8b17-2085be1342a5"
   },
   "source": [
    "## Revision History\n",
    "\n",
    "| #   | Date       | Action                                           | Modified by        |\n",
    "|-----|------------|--------------------------------------------------|--------------------|\n",
    "|     |            |                                                  |                    |\n",
    "| 030 | 2025-06-01 | Revert to Adadelta                               | rmaniego           |\n",
    "| 029 | 2025-06-01 | Apply LR Scheduler                               | rmaniego           |\n",
    "| 028 | 2025-06-01 | Decrease `EarlyStopping` patience                | rmaniego           |\n",
    "| 027 | 2025-06-01 | Increase validation partition (60:40)            | rmaniego           |\n",
    "| 026 | 2025-06-01 | Apply stronger regularization                    | rmaniego           |\n",
    "| 025 | 2025-06-01 | Tune hyperparameters                             | rmaniego           |\n",
    "| 024 | 2025-06-01 | Expand architecture                              | rmaniego           |\n",
    "| 023 | 2025-06-01 | Fix `Conv2D->BN->Activation` sequence            | rmaniego           |\n",
    "| 022 | 2025-06-01 | Fix map incorrect flipping                       | rmaniego           |\n",
    "| 021 | 2025-06-01 | Reduce dataset dimension                         | rmaniego           |\n",
    "| 020 | 2025-06-01 | Investigate dataset                              | rmaniego           |\n",
    "| 019 | 2025-05-30 | Fix differing evaluations                        | rmaniego           |\n",
    "| 018 | 2025-05-30 | Update test visualizations                       | rmaniego           |\n",
    "| 017 | 2025-05-27 | Fix `no-shear` dataset                           | rmaniego           |\n",
    "| 016 | 2025-05-27 | Tune hyperparameters                             | rmaniego           |\n",
    "| 015 | 2025-05-22 | Decrease initial LR                              | rmaniego           |\n",
    "| 014 | 2025-05-22 | Change to Tversky loss                           | rmaniego           |\n",
    "| 013 | 2025-05-21 | Add data augmentation                            | rmaniego           |\n",
    "| 012 | 2025-05-21 | Add data preprocessing                           | rmaniego           |\n",
    "| 011 | 2025-05-20 | Migrate to U-Net architecture                    | rmaniego           |\n",
    "| 010 | 2025-05-16 | Optimize architecture                            | rmaniego           |\n",
    "| 009 | 2025-05-16 | Improve architecture                             | rmaniego           |\n",
    "| 008 | 2025-05-15 | Fix model metrics                                | rmaniego           |\n",
    "| 007 | 2025-05-03 | Fix testing evaluation                           | rmaniego           |\n",
    "| 006 | 2025-05-03 | Fix dataset loader                               | rmaniego           |\n",
    "| 005 | 2025-05-03 | Fix segmentation dataset                         | rmaniego           |\n",
    "| 004 | 2025-04-10 | Fix architecture to match dataset                | rmaniego           |\n",
    "| 003 | 2025-04-10 | Update architecture base codes                   | rmaniego           |\n",
    "| 002 | 2025-04-09 | Prepare dataset                                  | rmaniego           |\n",
    "| 001 | 2025-03-29 | Create GitHub repository                         | rmaniego           |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e0c792-5a86-47d4-84e2-f77c1b7187a9",
   "metadata": {
    "id": "59e0c792-5a86-47d4-84e2-f77c1b7187a9"
   },
   "source": [
    "## Step 1. Mount Google Drive\n",
    "\n",
    "**Notes:**.\n",
    " - This requires GDrive permissions.\n",
    " - Update changes in local repository.\n",
    " - Re-run cell for every commit changes in the repository.\n",
    " - Colab is read only, unless set in GitHub FGPATs\n",
    "\n",
    "```python\n",
    "pip install jupyterlab\n",
    "pip install notebook\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "**GitHub Personal Access Tokens (PAT)**\n",
    "1. Go to `https://github.com/settings/tokens`.\n",
    "2. On the sidebar, select `Fine-grained tokens`.\n",
    "3. Fill-up appropriate details, limit read/write access.\n",
    "4. Copy generated `PAT` to local environment variables.\n",
    "5. Do the same to Google Colab secrets.\n",
    "6. Once expired, move the old repo in GDrive to trash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff34bdd-3fe1-4390-aebc-84f07d013ad9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ff34bdd-3fe1-4390-aebc-84f07d013ad9",
    "outputId": "88d02f33-a5ed-4d4a-facf-0399b65775e3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "github_fgpat = None\n",
    "live_on_colab = False\n",
    "environment_ready = False\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
    "\n",
    "try:\n",
    "    from google.colab import drive, userdata\n",
    "\n",
    "    drive.mount(\"/content/drive\")\n",
    "\n",
    "    live_on_colab = True\n",
    "    github_fgpat = userdata.get(\"ShearLineCNN\")\n",
    "    print(\"Running on Google Colaboratory...\")\n",
    "except ImportError:\n",
    "    print(\"Running locally...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cd2a48-9c27-4863-b23f-f635c33a0789",
   "metadata": {
    "id": "83cd2a48-9c27-4863-b23f-f635c33a0789"
   },
   "source": [
    "## Step 2. Check Colab Compute Engine Backend\n",
    "**Note:** Execute to verify HW accelerator allocation, use information on manuscript.\n",
    "\n",
    "HW accelerator availability may vary, so ensure that the session is timed and is connected to expected runtime environment in all iterations. Options include:\n",
    "1. NVIDIA A100 Tensor Core GPU - high-performance deep learning training (recommended).\n",
    "2. NVIDIA L4 Tensor Core GPU - optimized for AI inference tasks with high performance and efficiency (preferred during HP fine-tuning).\n",
    "3. NVIDIA T4 Tensor Core GPU - cost-effective, versatile, and suitable for a variety of tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b6346a-0cf0-45ae-adac-7ec8d8a08bcc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a6b6346a-0cf0-45ae-adac-7ec8d8a08bcc",
    "outputId": "f6df6a14-d840-4bd1-81de-ca62c6b3e5be"
   },
   "outputs": [],
   "source": [
    "if live_on_colab:\n",
    "    gpu_info = !nvidia-smi\n",
    "    gpu_info = \"\\n\".join(gpu_info)\n",
    "    if gpu_info.find(\"failed\") >= 0:\n",
    "        print(\"Not connected to a GPU\")\n",
    "    else:\n",
    "        print(gpu_info)\n",
    "\n",
    "    from psutil import virtual_memory\n",
    "\n",
    "    ram_gb = virtual_memory().total / 1e9\n",
    "    print(f\"Your runtime has {ram_gb:.1f} gigabytes of available RAM\")\n",
    "\n",
    "    if ram_gb < 20:\n",
    "        print(\"Not using a high-RAM runtime\")\n",
    "    else:\n",
    "        print(\"You are using a high-RAM runtime!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846e48d4-07e6-4f8d-a469-695cc37c6354",
   "metadata": {
    "id": "846e48d4-07e6-4f8d-a469-695cc37c6354"
   },
   "source": [
    "## Step 3. Change Working Directory\n",
    "\n",
    "**Notes:**  \n",
    "1. Before continuing, make sure you check your GDrive storage usage; cloning on limited storage may impact contents.  \n",
    "2. Execute cell to ensure the notebook is running under the latest version of project repository.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae29c92-27d9-4154-8951-328298daa982",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ae29c92-27d9-4154-8951-328298daa982",
    "outputId": "dba81774-c2c9-405f-bdbc-9ae8aeb43574"
   },
   "outputs": [],
   "source": [
    "if live_on_colab:\n",
    "    NB = \"/content/drive/MyDrive/Colab Notebooks\"\n",
    "    os.makedirs(NB, exist_ok=True)\n",
    "    os.chdir(NB)\n",
    "\n",
    "    def update_repo():\n",
    "\n",
    "        REPO = f\"{NB}/ShearLineCNN\"\n",
    "        if not os.path.isdir(REPO):\n",
    "            !git clone https://{github_fgpat}@github.com/rmaniego/ShearLineCNN.git\n",
    "            os.chdir(REPO)\n",
    "            return\n",
    "\n",
    "        os.chdir(REPO)\n",
    "        !git reset --hard HEAD\n",
    "        !git pull origin main\n",
    "\n",
    "    update_repo()\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6a8531-7976-4e6f-b943-4a72e056ed1d",
   "metadata": {
    "id": "1a6a8531-7976-4e6f-b943-4a72e056ed1d"
   },
   "source": [
    "## Step 4. Install Dependencies\n",
    "**Note:** Execute cell everytime the `Google Colab` runtime environment reconnected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dafb19c-b37d-4527-8f64-ab4e12efd6c8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7dafb19c-b37d-4527-8f64-ab4e12efd6c8",
    "outputId": "f96b3c20-6329-49a7-f438-3d59639b89f5"
   },
   "outputs": [],
   "source": [
    "if live_on_colab:\n",
    "    %pip install -U jupyterlab\n",
    "    %pip install -U notebook\n",
    "    %pip install -U opencv-python\n",
    "    %pip install -U scikit-learn\n",
    "    %pip install -U scikit-image\n",
    "    %pip install -U tensorflow\n",
    "    %pip install -U matplotlib\n",
    "    %pip install -U seaborn\n",
    "    %pip install -U cartopy\n",
    "print(\"Environment is ready...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e09b829-9e4e-405e-ba4c-cf01e76cde4b",
   "metadata": {
    "id": "0e09b829-9e4e-405e-ba4c-cf01e76cde4b"
   },
   "source": [
    "## Step 5: Import the Packages  \n",
    "\n",
    "import all third party libraries necessary for the ANN model to execute successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d9d045-7f6b-43af-9271-25872b31a013",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "73d9d045-7f6b-43af-9271-25872b31a013",
    "outputId": "9c1bb9aa-4ff7-4591-b292-a35cd2488c07"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\"os.fork()\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Your `PyDataset` class should call\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"warn\")\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from skimage.morphology import disk\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.ndimage import binary_dilation, binary_erosion\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Input, Conv2D, Activation, BatchNormalization, MaxPooling2D, Dropout, Conv2DTranspose, Concatenate, Cropping2D\n",
    "from tensorflow.keras.optimizers import Adadelta, AdamW\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "            print(e)\n",
    "    print(\"GPU detected. Running on GPU.\")\n",
    "else:\n",
    "    print(\"No GPU detected. Running on CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb73f5f4-ab60-487e-96a0-45c562c5688b",
   "metadata": {
    "id": "bb73f5f4-ab60-487e-96a0-45c562c5688b"
   },
   "source": [
    "## Step 6: Load Datasets  \n",
    "\n",
    "Load and prepare the training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2167f830-dd47-48bf-a7e5-496c1227cebf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2167f830-dd47-48bf-a7e5-496c1227cebf",
    "outputId": "c9341cf5-150c-4183-f06b-a886997a4f2e"
   },
   "outputs": [],
   "source": [
    "def load_features(source, target, category):\n",
    "    basenames, features, labels = [], [], []\n",
    "\n",
    "    sources = glob.glob(f\"{source}/{category}/*.json\")\n",
    "    for i, source_path in enumerate(sources):\n",
    "        filename = os.path.basename(source_path)\n",
    "\n",
    "        with open(source_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            data1 = np.array(json.load(file))\n",
    "\n",
    "        target_path = f\"{target}/{category}/{filename}\"\n",
    "        with open(target_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            data2 = np.array(json.load(file))\n",
    "\n",
    "        features.append(data1)\n",
    "        labels.append(data2)\n",
    "        basenames.append(filename)\n",
    "\n",
    "    return basenames, np.array(features), np.array(labels)\n",
    "\n",
    "def visualize_map(i, filename, features, true_mask, predicted_mask=None, fp_flag=False, figsize=(2, 1), extent=(115, 150, 5, 45)):\n",
    "\n",
    "    label = \" (FP)\" if fp_flag else \"\"\n",
    "    print(f\"#{i+1}: {filename}\")\n",
    "\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "    ax.set_extent(extent)\n",
    "    ax.set_facecolor(\"#303030\")\n",
    "    ax.coastlines(resolution=\"10m\", linewidth=0.5, color=\"k\")\n",
    "\n",
    "    ny, nx = features.shape[:2]\n",
    "    lon0 = np.linspace(extent[0], extent[1], nx)\n",
    "    lat0 = np.linspace(extent[2], extent[3], ny)\n",
    "    lon, lat = np.meshgrid(lon0, lat0)\n",
    "\n",
    "    ax.contourf(lon, lat, features, levels=20, cmap=\"coolwarm\", transform=ccrs.PlateCarree(), zorder=1)\n",
    "\n",
    "    true_overlay = np.ma.masked_where(true_mask == 0, true_mask)\n",
    "    ax.imshow(true_overlay, cmap=\"Greys\", alpha=0.8, extent=extent, transform=ccrs.PlateCarree(), zorder=2)\n",
    "\n",
    "    if predicted_mask is not None:\n",
    "        pred_overlay = np.ma.masked_where(predicted_mask == 0, predicted_mask)\n",
    "        ax.imshow(pred_overlay, cmap=\"Wistia\", alpha=0.5, extent=extent, transform=ccrs.PlateCarree(), zorder=3)\n",
    "\n",
    "    ax.set_title(f\"Spatial Map{label}\")\n",
    "    plt.show()\n",
    "    print(\"---\\n\")\n",
    "\n",
    "\n",
    "#################################################\n",
    "# Label Dilation (Widening)                     #\n",
    "#-----------------------------------------------#\n",
    "# Dilate shear line annotation to help model    #\n",
    "# converge faster. Since 1px is too thin        #\n",
    "# for the model to classify easiliy.            #\n",
    "#################################################\n",
    "\n",
    "def label_line_widening(labels_array, target_width=5):\n",
    "    \"\"\"\n",
    "    Modifies the line width in a batch of binary segmentation masks in-memory\n",
    "    to a specified target width using morphological dilation.\n",
    "    \"\"\"\n",
    "\n",
    "    if target_width < 1 or target_width % 2 == 0:\n",
    "        raise ValueError(\"target_width must be a positive odd integer (e.g., 1, 3, 5).\")\n",
    "\n",
    "    dilation_radius = (target_width - 1) // 2\n",
    "\n",
    "    if dilation_radius == 0:\n",
    "        return labels_array.astype(labels_array.dtype)\n",
    "\n",
    "    struct_elem = disk(dilation_radius)\n",
    "\n",
    "    modified_labels = np.zeros_like(labels_array, dtype=labels_array.dtype)\n",
    "    num_samples = labels_array.shape[0]\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        label_2d = labels_array[i].squeeze()\n",
    "\n",
    "        modified_label_2d = binary_dilation(label_2d, structure=struct_elem).astype(labels_array.dtype)\n",
    "\n",
    "        if len(labels_array.shape) == 4:\n",
    "            modified_labels[i] = np.expand_dims(modified_label_2d, axis=-1)\n",
    "        else:\n",
    "            modified_labels[i] = modified_label_2d\n",
    "\n",
    "    return modified_labels\n",
    "\n",
    "\n",
    "#################################################\n",
    "# Label Erosion (Shrinking)                     #\n",
    "#-----------------------------------------------#\n",
    "# Erode shear line annotation to approximate    #\n",
    "# original thin lines. Useful for reversing     #\n",
    "# prior dilation step for analysis or           #\n",
    "# visualization.                                #\n",
    "#################################################\n",
    "\n",
    "def label_line_shrinking(labels_array, target_width=5):\n",
    "    if target_width < 1 or target_width % 2 == 0:\n",
    "        raise ValueError(\"target_width must be a positive odd integer (e.g., 1, 3, 5).\")\n",
    "\n",
    "    erosion_radius = (target_width - 1) // 2\n",
    "\n",
    "    if erosion_radius == 0:\n",
    "        return labels_array.astype(labels_array.dtype)\n",
    "\n",
    "    struct_elem = disk(erosion_radius)\n",
    "\n",
    "    modified_labels = np.zeros_like(labels_array, dtype=labels_array.dtype)\n",
    "    num_samples = labels_array.shape[0]\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        label_2d = labels_array[i].squeeze()\n",
    "\n",
    "        modified_label_2d = binary_erosion(label_2d, structure=struct_elem).astype(labels_array.dtype)\n",
    "\n",
    "        if len(labels_array.shape) == 4:\n",
    "            modified_labels[i] = np.expand_dims(modified_label_2d, axis=-1)\n",
    "        else:\n",
    "            modified_labels[i] = modified_label_2d\n",
    "\n",
    "    return modified_labels\n",
    "\n",
    "\n",
    "basenames_list, features_list, labels_list = [], [], []\n",
    "\n",
    "categories = [\"no-shear\", \"shear\"]\n",
    "segmentation_source = \"data/segmentation/source\"\n",
    "segmentation_target = \"data/segmentation/target\"\n",
    "for category in categories:\n",
    "    basenames, features, labels = load_features(segmentation_source, segmentation_target, category)\n",
    "    if features.size > 0:\n",
    "        basenames_list.extend(basenames)\n",
    "        features_list.append(features)\n",
    "        labels_list.append(labels)\n",
    "\n",
    "features = np.vstack(features_list)\n",
    "labels = np.vstack(labels_list)\n",
    "basenames = np.array(basenames_list)\n",
    "\n",
    "#################################################\n",
    "# Perturbation-based Dataset Augmentation       #\n",
    "#################################################\n",
    "\n",
    "target_width = 7  # odd number\n",
    "labels = labels.astype(np.uint8)\n",
    "labels = label_line_widening(labels, target_width=target_width)\n",
    "\n",
    "test_ratio = 0.1\n",
    "target_total_size = 10_000\n",
    "train_val_target_size = int(target_total_size * (1 - test_ratio))\n",
    "\n",
    "binary_class_labels = np.array([1 if labels[i].any() else 0 for i in range(len(labels))])\n",
    "\n",
    "# 90:10 split for testing (using indices) with stratification\n",
    "total_indices = np.arange(len(basenames))\n",
    "train_val_indices, test_indices = train_test_split(\n",
    "    total_indices,\n",
    "    test_size=test_ratio,\n",
    "    shuffle=True,\n",
    "    stratify=binary_class_labels\n",
    ")\n",
    "\n",
    "features_train_val = features[train_val_indices]\n",
    "labels_train_val = labels[train_val_indices]\n",
    "basenames_train_val = basenames[train_val_indices]\n",
    "\n",
    "current_train_val_size = len(features_train_val)\n",
    "if current_train_val_size < train_val_target_size:\n",
    "    num_augmentations_needed = train_val_target_size - current_train_val_size\n",
    "\n",
    "    augmented_features_list = []\n",
    "    augmented_labels_list = []\n",
    "    augmented_basenames_list = []\n",
    "    noise_std_dev = 0.2\n",
    "\n",
    "    idx_to_augment = list(range(current_train_val_size))\n",
    "    random.shuffle(idx_to_augment)\n",
    "\n",
    "    aug_count = 0\n",
    "    while aug_count < num_augmentations_needed:\n",
    "        if not idx_to_augment:\n",
    "            idx_to_augment = list(range(current_train_val_size))\n",
    "            random.shuffle(idx_to_augment)\n",
    "\n",
    "        original_idx = idx_to_augment.pop(0)\n",
    "\n",
    "        original_feature = features_train_val[original_idx]\n",
    "        original_label = labels_train_val[original_idx]\n",
    "        original_basename = basenames_train_val[original_idx]\n",
    "\n",
    "        noise = np.random.normal(loc=0.0, scale=noise_std_dev, size=original_feature.shape)\n",
    "        aug_feature = original_feature + noise\n",
    "        aug_label = original_label\n",
    "\n",
    "        augmented_features_list.append(aug_feature)\n",
    "        augmented_labels_list.append(aug_label)\n",
    "        augmented_basenames_list.append(f\"{original_basename}_aug{aug_count}_noise{noise_std_dev}\")\n",
    "\n",
    "        aug_count += 1\n",
    "        if aug_count % 100 == 0:\n",
    "            print(f\"Generated {aug_count} augmented samples...\")\n",
    "\n",
    "    augmented_features = np.array(augmented_features_list)\n",
    "    augmented_labels = np.array(augmented_labels_list)\n",
    "    augmented_basenames = np.array(augmented_basenames_list)\n",
    "\n",
    "    features_train_val = np.concatenate((features_train_val, augmented_features), axis=0)\n",
    "    labels_train_val = np.concatenate((labels_train_val, augmented_labels), axis=0)\n",
    "    basenames_train_val = np.concatenate((basenames_train_val, augmented_basenames), axis=0)\n",
    "\n",
    "\n",
    "#################################################\n",
    "# Dataset Splitting                             #\n",
    "#################################################\n",
    "\n",
    "train_val_labels = np.array([1 if labels_train_val[i].any() else 0 for i in range(len(labels_train_val))])\n",
    "\n",
    "# 80:20 split for train-validation (on train-val set) with stratification\n",
    "train_val_size = len(features_train_val)\n",
    "indices_train, indices_val = train_test_split(\n",
    "    np.arange(train_val_size),\n",
    "    test_size=0.2,\n",
    "    shuffle=True,\n",
    "    stratify=train_val_labels\n",
    ")\n",
    "\n",
    "basenames_train = basenames_train_val[indices_train]\n",
    "features_train = features_train_val[indices_train]\n",
    "labels_train = labels_train_val[indices_train]\n",
    "\n",
    "basenames_val = basenames_train_val[indices_val]\n",
    "features_val = features_train_val[indices_val]\n",
    "labels_val = labels_train_val[indices_val]\n",
    "\n",
    "basenames_test = basenames[test_indices]\n",
    "features_test = features[test_indices]\n",
    "labels_test = labels[test_indices]\n",
    "\n",
    "n_train = len(features_train)\n",
    "n_val = len(features_val)\n",
    "n_test = len(features_test)\n",
    "\n",
    "print(f\"[Train Samples: {n_train}]\")\n",
    "# for i, (filename, features, true_mask) in enumerate(zip(basenames_train, features_train, labels_train)):\n",
    "#     visualize_map(i, filename, features, true_mask)\n",
    "\n",
    "print(f\"\\n[Validation Samples: {n_val}]\")\n",
    "# for i, (filename, features, true_mask) in enumerate(zip(basenames_val, features_val, labels_val)):\n",
    "#     visualize_map(i, filename, features, true_mask)\n",
    "\n",
    "print(f\"\\n[Test Samples: {n_test}]\")\n",
    "# for i, (filename, features, true_mask) in enumerate(zip(basenames_test, features_test, labels_test)):\n",
    "#     visualize_map(i, filename, features, true_mask)\n",
    "\n",
    "n_dataset = n_train + n_val + n_test\n",
    "print(f\"\\nTOTAL: {n_dataset}\")\n",
    "\n",
    "print(\"Dataset ready...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12eeb2b2-bb64-43cb-a35f-27c2ffc98e99",
   "metadata": {
    "id": "12eeb2b2-bb64-43cb-a35f-27c2ffc98e99"
   },
   "source": [
    "## Step 7: Define the Architecture  \n",
    "\n",
    "Define the structure of the convolutional neural network for shear line classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa709b18-4e1b-4ddc-b84f-2bf3d6702e6f",
   "metadata": {
    "id": "aa709b18-4e1b-4ddc-b84f-2bf3d6702e6f"
   },
   "outputs": [],
   "source": [
    "ALPHA = 0.95\n",
    "BETA = 0.05\n",
    "\n",
    "def tversky_index(y_true, y_pred, alpha=ALPHA, beta=BETA, smooth=K.epsilon()):\n",
    "    \"\"\"\n",
    "    Tversky index for binary segmentation.\n",
    "    -------------------------------------------------------------------\n",
    "    Salehi, S. S. M., Erdogmus, D., & Gholipour, A. (2017).\n",
    "    Tversky loss function for image segmentation\n",
    "    using 3D fully convolutional deep networks (No. arXiv:1706.05721).\n",
    "    arXiv. https://doi.org/10.48550/arXiv.1706.05721\n",
    "    \"\"\"\n",
    "    y_true_f = K.cast(K.flatten(y_true), \"float32\")\n",
    "    y_pred_f = K.cast(K.flatten(y_pred), \"float32\")\n",
    "    tp = K.sum(y_true_f * y_pred_f)\n",
    "    fp = K.sum((1 - y_true_f) * y_pred_f)\n",
    "    fn = K.sum(y_true_f * (1 - y_pred_f))\n",
    "    return (tp + smooth) / (tp + alpha * fp + beta * fn + smooth)\n",
    "\n",
    "def tversky_loss(y_true, y_pred):\n",
    "    \"\"\"Tversky loss (1 - Tversky index).\"\"\"\n",
    "    return 1 - tversky_index(y_true, y_pred)\n",
    "\n",
    "\n",
    "#################################################\n",
    "# Custom U-Net Architecture                     #\n",
    "#-----------------------------------------------#\n",
    "# Uses skip connections by concatenating        #\n",
    "# encoder outputs with corresponding decoder    #\n",
    "# layers to preserve spatial information and    #\n",
    "# improve segmentation accuracy.                #\n",
    "#################################################\n",
    "\n",
    "# Layer declarations\n",
    "convXa = Conv2D(32, 3, padding=\"same\", kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-03))\n",
    "bnXa = BatchNormalization()\n",
    "actXa = Activation(\"relu\")\n",
    "convXb = Conv2D(32, 3, padding=\"same\", kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-03))\n",
    "bnXb = BatchNormalization()\n",
    "actXb = Activation(\"relu\")\n",
    "poolX = MaxPooling2D(2, padding=\"same\")\n",
    "dropoutX = Dropout(0.3)\n",
    "\n",
    "conv0a = Conv2D(64, 3, padding=\"same\", kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-03))\n",
    "bn0a = BatchNormalization()\n",
    "act0a = Activation(\"relu\")\n",
    "conv0b = Conv2D(64, 3, padding=\"same\", kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-03))\n",
    "bn0b = BatchNormalization()\n",
    "act0b = Activation(\"relu\")\n",
    "pool0 = MaxPooling2D(2, padding=\"same\")\n",
    "dropout0 = Dropout(0.3)\n",
    "\n",
    "conv1a = Conv2D(128, 3, padding=\"same\", kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-03))\n",
    "bn1a = BatchNormalization()\n",
    "act1a = Activation(\"relu\")\n",
    "conv1b = Conv2D(128, 3, padding=\"same\", kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-03))\n",
    "bn1b = BatchNormalization()\n",
    "act1b = Activation(\"relu\")\n",
    "pool1 = MaxPooling2D(2, padding=\"same\")\n",
    "dropout1 = Dropout(0.3)\n",
    "\n",
    "conv2a = Conv2D(256, 3, padding=\"same\", kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-03))\n",
    "bn2a = BatchNormalization()\n",
    "act2a = Activation(\"relu\")\n",
    "conv2b = Conv2D(256, 3, padding=\"same\", kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-03))\n",
    "bn2b = BatchNormalization()\n",
    "act2b = Activation(\"relu\")\n",
    "pool2 = MaxPooling2D(2, padding=\"same\")\n",
    "dropout2 = Dropout(0.3)\n",
    "\n",
    "conv3a = Conv2D(512, 3, padding=\"same\", kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-03))\n",
    "bn3a = BatchNormalization()\n",
    "act3a = Activation(\"relu\")\n",
    "conv3b = Conv2D(512, 3, padding=\"same\", kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-03))\n",
    "bn3b = BatchNormalization()\n",
    "act3b = Activation(\"relu\")\n",
    "pool3 = MaxPooling2D(2, padding=\"same\")\n",
    "dropout3 = Dropout(0.3)\n",
    "\n",
    "bottleneck1a = Conv2D(1024, 3, padding=\"same\", kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-03))\n",
    "bn4a = BatchNormalization()\n",
    "act4a = Activation(\"relu\")\n",
    "bottleneck1b = Conv2D(1024, 3, padding=\"same\", kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-03))\n",
    "bn4b = BatchNormalization()\n",
    "act4b = Activation(\"relu\")\n",
    "bottleneck1c = Conv2D(1024, 3, padding=\"same\", kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-03))\n",
    "bn4c = BatchNormalization()\n",
    "act4c = Activation(\"relu\")\n",
    "dropout4 = Dropout(0.3)\n",
    "\n",
    "up1 = Conv2DTranspose(512, 2, strides=2, padding=\"same\")\n",
    "crop1 = Cropping2D(((1, 0), (1, 0)))\n",
    "concat1 = Concatenate()\n",
    "conv4a = Conv2D(512, 3, padding=\"same\", kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-03))\n",
    "bn5a = BatchNormalization()\n",
    "act5a = Activation(\"relu\")\n",
    "conv4b = Conv2D(512, 3, padding=\"same\", kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-03))\n",
    "bn5b = BatchNormalization()\n",
    "act5b = Activation(\"relu\")\n",
    "bn5 = BatchNormalization()\n",
    "dropout5 = Dropout(0.3)\n",
    "\n",
    "up2 = Conv2DTranspose(256, 2, strides=2, padding=\"same\")\n",
    "crop2 = Cropping2D(((1, 0), (0, 0)))\n",
    "concat2 = Concatenate()\n",
    "conv5a = Conv2D(256, 3, padding=\"same\", kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-03))\n",
    "bn6a = BatchNormalization()\n",
    "act6a = Activation(\"relu\")\n",
    "conv5b = Conv2D(256, 3, padding=\"same\", kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-03))\n",
    "bn6b = BatchNormalization()\n",
    "act6b = Activation(\"relu\")\n",
    "dropout6 = Dropout(0.3)\n",
    "\n",
    "up3 = Conv2DTranspose(128, 2, strides=2, padding=\"same\")\n",
    "crop3 = Cropping2D(((1, 0), (0, 0)))\n",
    "concat3 = Concatenate()\n",
    "conv6a = Conv2D(128, 3, padding=\"same\", kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-03))\n",
    "bn7a = BatchNormalization()\n",
    "act7a = Activation(\"relu\")\n",
    "conv6b = Conv2D(128, 3, padding=\"same\", kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-03))\n",
    "bn7b = BatchNormalization()\n",
    "act7b = Activation(\"relu\")\n",
    "dropout7 = Dropout(0.3)\n",
    "\n",
    "up4 = Conv2DTranspose(64, 2, strides=2, padding=\"same\")\n",
    "crop4 = Cropping2D(((1, 0), (1, 0)))\n",
    "concat4 = Concatenate()\n",
    "conv7a = Conv2D(64, 3, padding=\"same\", kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-03))\n",
    "bn8a = BatchNormalization()\n",
    "act8a = Activation(\"relu\")\n",
    "conv7b = Conv2D(64, 3, padding=\"same\", kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-03))\n",
    "bn8b = BatchNormalization()\n",
    "act8b = Activation(\"relu\")\n",
    "dropout8 = Dropout(0.3)\n",
    "\n",
    "up5 = Conv2DTranspose(32, 2, strides=2, padding=\"same\")\n",
    "crop5 = Cropping2D(((1, 0), (1, 0)))\n",
    "concat5 = Concatenate()\n",
    "conv8a = Conv2D(32, 3, padding=\"same\", kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-03))\n",
    "bn9a = BatchNormalization()\n",
    "act9a = Activation(\"relu\")\n",
    "conv8b = Conv2D(32, 3, padding=\"same\", kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-03))\n",
    "bn9b = BatchNormalization()\n",
    "act9b = Activation(\"relu\")\n",
    "dropout9 = Dropout(0.3)\n",
    "\n",
    "final = Conv2D(1, 1, activation=\"sigmoid\")\n",
    "\n",
    "\n",
    "#################################################\n",
    "# Forward Pass with concatenation.              #\n",
    "#################################################\n",
    "\n",
    "inputs = Input(shape=(161, 141, 1))\n",
    "\n",
    "outputsX = convXa(inputs)\n",
    "outputsX = bnXa(outputsX)\n",
    "outputsX = actXa(outputsX)\n",
    "outputsX = convXb(outputsX)\n",
    "outputsX = bnXb(outputsX)\n",
    "outputsX = actXb(outputsX)\n",
    "pooledX = poolX(outputsX)\n",
    "pooledX = dropoutX(pooledX)\n",
    "\n",
    "outputs0 = conv0a(pooledX)\n",
    "outputs0 = bn0a(outputs0)\n",
    "outputs0 = act0a(outputs0)\n",
    "outputs0 = conv0b(outputs0)\n",
    "outputs0 = bn0b(outputs0)\n",
    "outputs0 = act0b(outputs0)\n",
    "pooled0 = pool0(outputs0)\n",
    "pooled0 = dropout0(pooled0)\n",
    "\n",
    "outputs1 = conv1a(pooled0)\n",
    "outputs1 = bn1a(outputs1)\n",
    "outputs1 = act1a(outputs1)\n",
    "outputs1 = conv1b(outputs1)\n",
    "outputs1 = bn1b(outputs1)\n",
    "outputs1 = act1b(outputs1)\n",
    "pooled1 = pool1(outputs1)\n",
    "pooled1 = dropout1(pooled1)\n",
    "\n",
    "outputs2 = conv2a(pooled1)\n",
    "outputs2 = bn2a(outputs2)\n",
    "outputs2 = act2a(outputs2)\n",
    "outputs2 = conv2b(outputs2)\n",
    "outputs2 = bn2b(outputs2)\n",
    "outputs2 = act2b(outputs2)\n",
    "pooled2 = pool2(outputs2)\n",
    "pooled2 = dropout2(pooled2)\n",
    "\n",
    "outputs3 = conv3a(pooled2)\n",
    "outputs3 = bn3a(outputs3)\n",
    "outputs3 = act3a(outputs3)\n",
    "outputs3 = conv3b(outputs3)\n",
    "outputs3 = bn3b(outputs3)\n",
    "outputs3 = act3b(outputs3)\n",
    "pooled3 = pool3(outputs3)\n",
    "pooled3 = dropout3(pooled3)\n",
    "\n",
    "outputs = bottleneck1a(pooled3)\n",
    "outputs = bn4a(outputs)\n",
    "outputs = act4a(outputs)\n",
    "outputs = bottleneck1b(outputs)\n",
    "outputs = bn4b(outputs)\n",
    "outputs = act4b(outputs)\n",
    "outputs = bottleneck1c(outputs)\n",
    "outputs = bn4c(outputs)\n",
    "outputs = act4c(outputs)\n",
    "outputs = dropout4(outputs)\n",
    "\n",
    "outputs = up1(outputs)\n",
    "outputs = crop1(outputs)\n",
    "outputs = concat1([outputs, outputs3])\n",
    "outputs = conv4a(outputs)\n",
    "outputs = bn5a(outputs)\n",
    "outputs = act5a(outputs)\n",
    "outputs = conv4b(outputs)\n",
    "outputs = bn5b(outputs)\n",
    "outputs = act5b(outputs)\n",
    "outputs = dropout5(outputs)\n",
    "\n",
    "outputs = up2(outputs)\n",
    "outputs = crop2(outputs)\n",
    "outputs = concat2([outputs, outputs2])\n",
    "outputs = conv5a(outputs)\n",
    "outputs = bn6a(outputs)\n",
    "outputs = act6a(outputs)\n",
    "outputs = conv5b(outputs)\n",
    "outputs = bn6b(outputs)\n",
    "outputs = act6b(outputs)\n",
    "outputs = dropout6(outputs)\n",
    "\n",
    "outputs = up3(outputs)\n",
    "outputs = crop3(outputs)\n",
    "outputs = concat3([outputs, outputs1])\n",
    "outputs = conv6a(outputs)\n",
    "outputs = bn7a(outputs)\n",
    "outputs = act7a(outputs)\n",
    "outputs = conv6b(outputs)\n",
    "outputs = bn7b(outputs)\n",
    "outputs = act7b(outputs)\n",
    "outputs = dropout7(outputs)\n",
    "\n",
    "outputs = up4(outputs)\n",
    "outputs = crop4(outputs)\n",
    "outputs = concat4([outputs, outputs0])\n",
    "outputs = conv7a(outputs)\n",
    "outputs = bn8a(outputs)\n",
    "outputs = act8a(outputs)\n",
    "outputs = conv7b(outputs)\n",
    "outputs = bn8b(outputs)\n",
    "outputs = act8b(outputs)\n",
    "outputs = dropout8(outputs)\n",
    "\n",
    "outputs = up5(outputs)\n",
    "outputs = crop5(outputs)\n",
    "outputs = concat5([outputs, outputsX])\n",
    "outputs = conv8a(outputs)\n",
    "outputs = bn9a(outputs)\n",
    "outputs = act9a(outputs)\n",
    "outputs = conv8b(outputs)\n",
    "outputs = bn9b(outputs)\n",
    "outputs = act9b(outputs)\n",
    "outputs = dropout9(outputs)\n",
    "\n",
    "outputs = final(outputs)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# Compile Model\n",
    "optimizer = Adadelta(learning_rate=1.0, rho=0.95)  # ρ = 0.9-0.95\n",
    "model.compile(optimizer=optimizer, loss=tversky_loss, metrics=[tversky_index])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "MODELS = \"models\"\n",
    "ARCHITECTURE = \"UNet\"\n",
    "model_path = f\"{MODELS}/shearline.{ARCHITECTURE}.png\"\n",
    "\n",
    "plot_model(model, to_file=model_path, show_shapes=False)\n",
    "Image(model_path, width=300)\n",
    "\n",
    "# Notes: 2025-05-15\n",
    "# The issue here is that BCE is not reliable when there is a class imbalance: where 0s dominates 1s across spatial maps.\n",
    "# After a series of experimentations, Dice shown inability to suppress false positives.\n",
    "# We found a more appropriate option, which is the Tversky Loss.\n",
    "# It is generalization of Dice and allows flexible penalization of FP and FN.\n",
    "# So, this loss can further refine the predictions by explicitly reducing FP.\n",
    "# Dice is just a special case of Tversky when α = β = 0.5; Tversky = TP / (TP + α*FP + β*FN)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489f68ca-7e79-46e5-a966-4fd7ac38d62d",
   "metadata": {
    "id": "489f68ca-7e79-46e5-a966-4fd7ac38d62d"
   },
   "source": [
    "### Step 8: Train the Model  \n",
    "\n",
    "> Feed the training-val dataset to the compiled CNN model.  \n",
    "\n",
    "**Note:** Re-compile model again before running this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d37089-07d2-47e2-ab25-dd2553dc96c2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "07d37089-07d2-47e2-ab25-dd2553dc96c2",
    "outputId": "2ff7fbaa-7e25-4c1d-cc17-451ae90175fc"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "ANALYSIS = \"analysis\"\n",
    "DATASET = \"data\"\n",
    "TEST = \"test\"\n",
    "\n",
    "os.makedirs(MODELS, exist_ok=True)\n",
    "os.makedirs(ANALYSIS, exist_ok=True)\n",
    "\n",
    "training_timestamp = int(time.time())\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=20, restore_best_weights=True)\n",
    "history = model.fit(\n",
    "    features_train,\n",
    "    labels_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(features_val, labels_val),\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "training_duration = (int(time.time()) - training_timestamp) / 60\n",
    "print(f\"Training completed in {training_duration:.2f} minutes.\")\n",
    "\n",
    "\n",
    "fullpath = f\"{MODELS}/shearline.{ARCHITECTURE}_{training_timestamp}.keras\"\n",
    "model.save(fullpath)\n",
    "\n",
    "with open(f\"{ANALYSIS}/metrics_{training_timestamp}.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"loss\": history.history[\"loss\"],\n",
    "        \"tversky_index\": history.history[\"tversky_index\"],\n",
    "        \"val_loss\": history.history[\"val_loss\"],\n",
    "        \"val_tversky_index\": history.history[\"val_tversky_index\"]\n",
    "    }, f, indent=4)\n",
    "\n",
    "print(f\"Model training complete and saved to '{fullpath}'\")\n",
    "print(f\"Training and validation metrics saved to '{ANALYSIS}/metrics_{training_timestamp}.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340c1868-d11e-4eb8-a7b9-7395f672c51a",
   "metadata": {
    "id": "340c1868-d11e-4eb8-a7b9-7395f672c51a"
   },
   "source": [
    "## Step 9: Generate Training Analysis  \n",
    "\n",
    "**Metrics Definitions**\n",
    "* Loss is computed based on how far each prediction is from the ground truth, specifically using Tversky Loss.\n",
    "* Tversky Index is a generalized Dice Coefficient and is a type of accuracy, which allows flexible penalization of FP and FN--best for imbalanced datasets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838ae30e-0e40-4505-aa94-d15ac9f3b35a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "838ae30e-0e40-4505-aa94-d15ac9f3b35a",
    "outputId": "4151532c-574f-404e-a1e7-c6cfcf1c5f6c"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "with open(f\"{ANALYSIS}/metrics_{training_timestamp}.json\", \"r\") as f:\n",
    "    metrics = json.load(f)\n",
    "\n",
    "epochs = range(1, len(metrics[\"loss\"]) + 1)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.lineplot(x=epochs, y=metrics[\"loss\"], label=\"Training Tversky Loss\", color=\"blue\")\n",
    "sns.lineplot(x=epochs, y=metrics[\"val_loss\"], label=\"Validation Tversky Loss\", color=\"orange\")\n",
    "plt.title(\"Tversky Loss vs. Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Tversky Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(f\"{ANALYSIS}/tversky_loss_plot_{training_timestamp}.png\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.lineplot(x=epochs, y=metrics[\"tversky_index\"], label=\"Training Tversky Index\", color=\"green\")\n",
    "sns.lineplot(x=epochs, y=metrics[\"val_tversky_index\"], label=\"Validation Tversky Index\", color=\"red\")\n",
    "plt.title(\"Tversky Index vs. Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Tversky Index\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(f\"{ANALYSIS}/tversky_index_plot_{training_timestamp}.png\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPlots saved to {ANALYSIS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04709c7-826a-405e-aad1-8dcf59acad51",
   "metadata": {
    "id": "b04709c7-826a-405e-aad1-8dcf59acad51"
   },
   "source": [
    "## Step 10: Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3994e241-f4d0-4587-947d-9eb50bdcb7a6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3994e241-f4d0-4587-947d-9eb50bdcb7a6",
    "outputId": "82057aa7-5848-4d96-80cd-133c035cd00a"
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "results = model.predict(features_test, verbose=1)\n",
    "prediction_duration = time.time() - start_time\n",
    "image_prediction_time = prediction_duration / len(features_test)\n",
    "\n",
    "predictions = (results.squeeze(-1) > 0.5).astype(\"int32\")\n",
    "predictions_flat = predictions.flatten()\n",
    "labels_test_flat = labels_test.flatten()\n",
    "\n",
    "print(f\"Total prediction time: {prediction_duration:.4f} seconds\")\n",
    "print(f\"Time per spatial map: {image_prediction_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bd7c38-2cc2-4ea2-b4db-26f7663063ba",
   "metadata": {
    "id": "c5bd7c38-2cc2-4ea2-b4db-26f7663063ba"
   },
   "source": [
    "## Step 11: Display the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fca587a-9287-453d-b86c-1023af269c92",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8fca587a-9287-453d-b86c-1023af269c92",
    "outputId": "e4ea8e9c-33ad-4c46-b141-d2ea86968f39",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "false_positives = []\n",
    "for i, (filename, true_mask, predicted_mask, features) in enumerate(zip(basenames_test, labels_test, predictions, features_test)):\n",
    "    fp_flag = np.all(true_mask == 0) and np.any(predicted_mask != 0)\n",
    "    if fp_flag:\n",
    "        false_positives.append(filename)\n",
    "    visualize_map(i, filename, features, true_mask, predicted_mask, fp_flag, figsize=(6, 4))\n",
    "\n",
    "total_fp = len(false_positives)\n",
    "print(f\"\\n[False Positives ({total_fp})]\")\n",
    "for filename in false_positives:\n",
    "    print(filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479984a7-a80d-49ab-96d5-e6ab8bb9e9c9",
   "metadata": {
    "id": "479984a7-a80d-49ab-96d5-e6ab8bb9e9c9"
   },
   "source": [
    "## Step 12: Region-based Segmentation Evaluation\n",
    "\n",
    "> Evaluate the spatial map accuracy of predicted shear line binary mask using segmentation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb56799-76ed-45a8-96d4-5dfcd015577f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 550
    },
    "id": "5cb56799-76ed-45a8-96d4-5dfcd015577f",
    "outputId": "52486f39-395a-461b-d5ed-27fe9e6209fc"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "tverskys = []\n",
    "\n",
    "for t_mask, p_mask in zip(labels_test, predictions):\n",
    "    t_mask = t_mask.reshape(161, 141)\n",
    "    p_mask = p_mask.reshape(161, 141)\n",
    "\n",
    "    tverskys.append(tversky_index(t_mask, p_mask))\n",
    "\n",
    "tversky_score = np.mean(tverskys)\n",
    "\n",
    "true_all = np.concatenate([t.flatten() for t in labels_test])\n",
    "pred_all = np.concatenate([p.flatten() for p in predictions])\n",
    "\n",
    "cm = confusion_matrix(true_all, pred_all, labels=[0, 1])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"No Shear Line\", \"Shear Line\"])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "disp.plot(cmap=\"magma\", ax=ax)\n",
    "plt.title(f\"Tversky Index={tversky_score:.4f}\")\n",
    "plt.savefig(f\"{ANALYSIS}/confusion_matrix_{training_timestamp}.png\")\n",
    "plt.show()\n",
    "\n",
    "# Landis & Koch (1977) scale (based on Kappa Scale)\n",
    "# with overlapping bounds to ensure proper interpretation\n",
    "interpretations = {\n",
    "    (-1.0, 0.00): \"Poor agreement\",\n",
    "    (0.00, 0.21): \"Slight agreement\",\n",
    "    (0.21, 0.41): \"Fair agreement\",\n",
    "    (0.41, 0.61): \"Moderate agreement\",\n",
    "    (0.61, 0.81): \"Substantial agreement\",\n",
    "    (0.81, 1.01): \"Almost perfect agreement\"\n",
    "}\n",
    "\n",
    "for score_range, interpretation in interpretations.items():\n",
    "    if score_range[0] <= tversky_score < score_range[1]:\n",
    "        print(f\"Tversky Interpretation: {interpretation}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f929d9-23ff-43c4-9d22-5363a1cf06ad",
   "metadata": {
    "id": "06f929d9-23ff-43c4-9d22-5363a1cf06ad"
   },
   "source": [
    "# Step 13: Duplicate Notebook  \n",
    "\n",
    "**Note:** Manually save first before duplicating the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e931bd5a-682c-46bc-abfe-69bd6aa3088c",
   "metadata": {
    "id": "e931bd5a-682c-46bc-abfe-69bd6aa3088c",
    "outputId": "587f7698-0033-4320-c944-de423ca106ca",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "VALIDATIONS = \"validations\"\n",
    "os.makedirs(VALIDATIONS, exist_ok=True)\n",
    "\n",
    "filename = \"ShearLineCNN.ipynb\"\n",
    "with open(filename, \"r\", encoding=\"utf-8\") as src:\n",
    "    contents = src.read()\n",
    "    checkpoint = f\"{VALIDATIONS}/{filename}\".replace(\".ipynb\", f\"_{training_timestamp}.ipynb\")\n",
    "    with open(checkpoint, \"w\", encoding=\"utf-8\") as dest:\n",
    "        dest.write(contents)\n",
    "        print(f\"Checkpoint was created at '{checkpoint}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a631ad5d-420c-4996-b772-1d6df7cac18d",
   "metadata": {
    "id": "a631ad5d-420c-4996-b772-1d6df7cac18d"
   },
   "source": [
    "> End of code."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
