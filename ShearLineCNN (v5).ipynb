{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae5d8f43-6828-4068-be52-b1695b6f0c43",
   "metadata": {
    "id": "ae5d8f43-6828-4068-be52-b1695b6f0c43"
   },
   "source": [
    "# ShearLineCNN\n",
    "> Shear Line Classification using CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db17189e-3c33-4faa-8b17-2085be1342a5",
   "metadata": {
    "id": "db17189e-3c33-4faa-8b17-2085be1342a5"
   },
   "source": [
    "## Revision History\n",
    "\n",
    "| #   | Date       | Action                                           | Modified by        |\n",
    "|-----|------------|--------------------------------------------------|--------------------|\n",
    "|     |            |                                                  |                    |\n",
    "| 015 | 2025-05-22 | Decrease initial LR                              | rmaniego           |\n",
    "| 014 | 2025-05-22 | Change to Tversky loss                           | rmaniego           |\n",
    "| 013 | 2025-05-21 | Add data augmentation                            | rmaniego           |\n",
    "| 012 | 2025-05-21 | Add data preprocessing                           | rmaniego           |\n",
    "| 011 | 2025-05-20 | Migrate to U-Net architecture                    | rmaniego           |\n",
    "| 010 | 2025-05-16 | Optimize architecture                            | rmaniego           |\n",
    "| 009 | 2025-05-16 | Improve architecture                             | rmaniego           |\n",
    "| 008 | 2025-05-15 | Fix model metrics                                | rmaniego           |\n",
    "| 007 | 2025-05-03 | Fix testing evaluation                           | rmaniego           |\n",
    "| 006 | 2025-05-03 | Fix dataset loader                               | rmaniego           |\n",
    "| 005 | 2025-05-03 | Fix segmentation dataset                         | rmaniego           |\n",
    "| 004 | 2025-04-10 | Fix architecture to match dataset                | rmaniego           |\n",
    "| 003 | 2025-04-10 | Update architecture base codes                   | rmaniego           |\n",
    "| 002 | 2025-04-09 | Prepare dataset                                  | rmaniego           |\n",
    "| 001 | 2025-03-29 | Create GitHub repository                         | rmaniego           |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e0c792-5a86-47d4-84e2-f77c1b7187a9",
   "metadata": {
    "id": "59e0c792-5a86-47d4-84e2-f77c1b7187a9"
   },
   "source": [
    "## Step 1. Mount Google Drive\n",
    "\n",
    "**Notes:**.\n",
    " - This requires GDrive permissions.\n",
    " - Update changes in local repository.\n",
    " - Re-run cell for every commit changes in the repository.\n",
    " - Colab is read only, unless set in GitHub FGPATs\n",
    "\n",
    "```python\n",
    "pip install jupyterlab\n",
    "pip install notebook\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "**GitHub Personal Access Tokens (PAT)**\n",
    "1. Go to `https://github.com/settings/tokens`.\n",
    "2. On the sidebar, select `Fine-grained tokens`.\n",
    "3. Fill-up appropriate details, limit read/write access.\n",
    "4. Copy generated `PAT` to local environment variables.\n",
    "5. Do the same to Google Colab secrets.\n",
    "6. Once expired, move the old repo in GDrive to trash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff34bdd-3fe1-4390-aebc-84f07d013ad9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ff34bdd-3fe1-4390-aebc-84f07d013ad9",
    "outputId": "88d02f33-a5ed-4d4a-facf-0399b65775e3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "github_fgpat = None\n",
    "live_on_colab = False\n",
    "environment_ready = False\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
    "\n",
    "try:\n",
    "    from google.colab import drive, userdata\n",
    "\n",
    "    drive.mount(\"/content/drive\")\n",
    "\n",
    "    live_on_colab = True\n",
    "    github_fgpat = userdata.get(\"ShearLineCNN\")\n",
    "    print(\"Running on Google Colaboratory...\")\n",
    "except ImportError:\n",
    "    print(\"Running locally...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cd2a48-9c27-4863-b23f-f635c33a0789",
   "metadata": {
    "id": "83cd2a48-9c27-4863-b23f-f635c33a0789"
   },
   "source": [
    "## Step 2. Check Colab Compute Engine Backend\n",
    "**Note:** Execute to verify HW accelerator allocation, use information on manuscript.\n",
    "\n",
    "HW accelerator availability may vary, so ensure that the session is timed and is connected to expected runtime environment in all iterations. Options include:\n",
    "1. NVIDIA A100 Tensor Core GPU - high-performance deep learning training (recommended).\n",
    "2. NVIDIA L4 Tensor Core GPU - optimized for AI inference tasks with high performance and efficiency (preferred during HP fine-tuning).\n",
    "3. NVIDIA T4 Tensor Core GPU - cost-effective, versatile, and suitable for a variety of tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b6346a-0cf0-45ae-adac-7ec8d8a08bcc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a6b6346a-0cf0-45ae-adac-7ec8d8a08bcc",
    "outputId": "f6df6a14-d840-4bd1-81de-ca62c6b3e5be"
   },
   "outputs": [],
   "source": [
    "if live_on_colab:\n",
    "    gpu_info = !nvidia-smi\n",
    "    gpu_info = \"\\n\".join(gpu_info)\n",
    "    if gpu_info.find(\"failed\") >= 0:\n",
    "        print(\"Not connected to a GPU\")\n",
    "    else:\n",
    "        print(gpu_info)\n",
    "\n",
    "    from psutil import virtual_memory\n",
    "\n",
    "    ram_gb = virtual_memory().total / 1e9\n",
    "    print(f\"Your runtime has {ram_gb:.1f} gigabytes of available RAM\")\n",
    "\n",
    "    if ram_gb < 20:\n",
    "        print(\"Not using a high-RAM runtime\")\n",
    "    else:\n",
    "        print(\"You are using a high-RAM runtime!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846e48d4-07e6-4f8d-a469-695cc37c6354",
   "metadata": {
    "id": "846e48d4-07e6-4f8d-a469-695cc37c6354"
   },
   "source": [
    "## Step 3. Change Working Directory\n",
    "\n",
    "**Notes:**  \n",
    "1. Before continuing, make sure you check your GDrive storage usage; cloning on limited storage may impact contents.  \n",
    "2. Execute cell to ensure the notebook is running under the latest version of project repository.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae29c92-27d9-4154-8951-328298daa982",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ae29c92-27d9-4154-8951-328298daa982",
    "outputId": "dba81774-c2c9-405f-bdbc-9ae8aeb43574"
   },
   "outputs": [],
   "source": [
    "if live_on_colab:\n",
    "    NB = \"/content/drive/MyDrive/Colab Notebooks\"\n",
    "    os.makedirs(NB, exist_ok=True)\n",
    "    os.chdir(NB)\n",
    "\n",
    "    def update_repo():\n",
    "\n",
    "        REPO = f\"{NB}/ShearLineCNN2\"\n",
    "        if not os.path.isdir(REPO):\n",
    "            !git clone https://{github_fgpat}@github.com/rmaniego/ShearLineCNN.git ShearLineCNN2\n",
    "            os.chdir(REPO)\n",
    "            return\n",
    "\n",
    "        os.chdir(REPO)\n",
    "        !git reset --hard HEAD\n",
    "        !git pull origin main\n",
    "\n",
    "    update_repo()\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6a8531-7976-4e6f-b943-4a72e056ed1d",
   "metadata": {
    "id": "1a6a8531-7976-4e6f-b943-4a72e056ed1d"
   },
   "source": [
    "## Step 4. Install Dependencies\n",
    "**Note:** Execute cell everytime the `Google Colab` runtime environment reconnected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dafb19c-b37d-4527-8f64-ab4e12efd6c8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7dafb19c-b37d-4527-8f64-ab4e12efd6c8",
    "outputId": "f96b3c20-6329-49a7-f438-3d59639b89f5"
   },
   "outputs": [],
   "source": [
    "if live_on_colab:\n",
    "    %pip install -U jupyterlab\n",
    "    %pip install -U notebook\n",
    "    %pip install -U opencv-python\n",
    "    %pip install -U scikit-learn\n",
    "    %pip install -U scikit-image\n",
    "    %pip install -U matplotlib\n",
    "    %pip install -U seaborn\n",
    "    %pip install -U tensorflow\n",
    "    %pip install -U tabulate\n",
    "print(\"Environment is ready...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e09b829-9e4e-405e-ba4c-cf01e76cde4b",
   "metadata": {
    "id": "0e09b829-9e4e-405e-ba4c-cf01e76cde4b"
   },
   "source": [
    "## Step 5: Import the Packages  \n",
    "\n",
    "import all third party libraries necessary for the ANN model to execute successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d9d045-7f6b-43af-9271-25872b31a013",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "73d9d045-7f6b-43af-9271-25872b31a013",
    "outputId": "9c1bb9aa-4ff7-4591-b292-a35cd2488c07"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\"os.fork()\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Your `PyDataset` class should call\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"warn\")\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from skimage.morphology import disk\n",
    "from scipy.ndimage import binary_dilation\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping  #, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, BatchNormalization, Dropout, Conv2DTranspose, UpSampling2D, Concatenate, Cropping2D\n",
    "from tensorflow.keras.optimizers import Adadelta, AdamW, Lion, RMSprop, SGD\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecayRestarts\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "            print(e)\n",
    "    print(\"GPU detected. Running on GPU.\")\n",
    "else:\n",
    "    print(\"No GPU detected. Running on CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb73f5f4-ab60-487e-96a0-45c562c5688b",
   "metadata": {
    "id": "bb73f5f4-ab60-487e-96a0-45c562c5688b"
   },
   "source": [
    "## Step 6: Load Datasets  \n",
    "\n",
    "Load and prepare the training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2167f830-dd47-48bf-a7e5-496c1227cebf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2167f830-dd47-48bf-a7e5-496c1227cebf",
    "outputId": "c9341cf5-150c-4183-f06b-a886997a4f2e"
   },
   "outputs": [],
   "source": [
    "def load_features(source, target, category):\n",
    "    basenames, features, labels = [], [], []\n",
    "\n",
    "    sources = glob.glob(f\"{source}/{category}/*.json\")\n",
    "    for i, source_path in enumerate(sources):\n",
    "        filename = os.path.basename(source_path)\n",
    "\n",
    "        with open(source_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            data1 = np.array(json.load(file))\n",
    "\n",
    "        target_path = f\"{target}/{category}/{filename}\"\n",
    "        with open(target_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            data2 = np.array(json.load(file))\n",
    "\n",
    "        features.append(data1)\n",
    "        labels.append(data2)\n",
    "        basenames.append(filename)\n",
    "\n",
    "    return basenames, np.array(features), np.array(labels)\n",
    "\n",
    "\n",
    "#################################################\n",
    "# Label Dilation (Widening)                     #\n",
    "#-----------------------------------------------#\n",
    "# Dilate shear line annotation to help model    #\n",
    "# converge faster. Since 1px is too thin        #\n",
    "# for the model to classify easiliy.            #\n",
    "#################################################\n",
    "\n",
    "def label_line_widening(labels_array, target_width=5):\n",
    "    \"\"\"\n",
    "    Modifies the line width in a batch of binary segmentation masks in-memory\n",
    "    to a specified target width using morphological dilation.\n",
    "    \"\"\"\n",
    "\n",
    "    if target_width < 1 or target_width % 2 == 0:\n",
    "        raise ValueError(\"target_width must be a positive odd integer (e.g., 1, 3, 5).\")\n",
    "\n",
    "    dilation_radius = (target_width - 1) // 2\n",
    "\n",
    "    if dilation_radius == 0:\n",
    "        return labels_array.astype(labels_array.dtype)\n",
    "\n",
    "    struct_elem = disk(dilation_radius)\n",
    "\n",
    "    modified_labels = np.zeros_like(labels_array, dtype=labels_array.dtype)\n",
    "    num_samples = labels_array.shape[0]\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        label_2d = labels_array[i].squeeze()\n",
    "\n",
    "        modified_label_2d = binary_dilation(label_2d, structure=struct_elem).astype(labels_array.dtype)\n",
    "\n",
    "        if len(labels_array.shape) == 4:\n",
    "            modified_labels[i] = np.expand_dims(modified_label_2d, axis=-1)\n",
    "        else:\n",
    "            modified_labels[i] = modified_label_2d\n",
    "\n",
    "    return modified_labels\n",
    "\n",
    "\n",
    "basenames_list, features_list, labels_list = [], [], []\n",
    "\n",
    "categories = [\"no-shear\", \"shear\"]\n",
    "segmentation_source = \"data/segmentation/source\"\n",
    "segmentation_target = \"data/segmentation/target\"\n",
    "for category in categories:\n",
    "    basenames, features, labels = load_features(segmentation_source, segmentation_target, category)\n",
    "    if features.size > 0:\n",
    "        basenames_list.extend(basenames)\n",
    "        features_list.append(features)\n",
    "        labels_list.append(labels)\n",
    "\n",
    "features = np.vstack(features_list)\n",
    "labels = np.vstack(labels_list)\n",
    "basenames = np.array(basenames_list)\n",
    "\n",
    "target_width = 3\n",
    "labels = labels.astype(np.uint8)\n",
    "labels = label_line_widening(labels, target_width=target_width)\n",
    "\n",
    "\n",
    "#################################################\n",
    "# Perturbation-based Dataset Augmentation       #\n",
    "#################################################\n",
    "\n",
    "test_ratio = 0.1\n",
    "target_total_size = 1000\n",
    "train_val_target_size = int(target_total_size * (1 - test_ratio))\n",
    "\n",
    "binary_class_labels = np.array([1 if labels[i].any() else 0 for i in range(len(labels))])\n",
    "\n",
    "# 90:10 split for testing (using indices) with stratification\n",
    "total_indices = np.arange(len(basenames))\n",
    "train_val_indices, test_indices = train_test_split(\n",
    "    total_indices,\n",
    "    test_size=test_ratio,\n",
    "    shuffle=True,\n",
    "    stratify=binary_class_labels\n",
    ")\n",
    "\n",
    "features_train_val = features[train_val_indices]\n",
    "labels_train_val = labels[train_val_indices]\n",
    "basenames_train_val = basenames[train_val_indices]\n",
    "\n",
    "\"\"\"\n",
    "current_train_val_size = len(features_train_val)\n",
    "if current_train_val_size < train_val_target_size:\n",
    "    num_augmentations_needed = train_val_target_size - current_train_val_size\n",
    "\n",
    "    augmented_features_list = []\n",
    "    augmented_labels_list = []\n",
    "    augmented_basenames_list = []\n",
    "    noise_std_dev = 0.5\n",
    "\n",
    "    idx_to_augment = list(range(current_train_val_size))\n",
    "    random.shuffle(idx_to_augment)\n",
    "\n",
    "    aug_count = 0\n",
    "    while aug_count < num_augmentations_needed:\n",
    "        if not idx_to_augment:\n",
    "            idx_to_augment = list(range(current_train_val_size))\n",
    "            random.shuffle(idx_to_augment)\n",
    "\n",
    "        original_idx = idx_to_augment.pop(0)\n",
    "\n",
    "        original_feature = features_train_val[original_idx]\n",
    "        original_label = labels_train_val[original_idx]\n",
    "        original_basename = basenames_train_val[original_idx]\n",
    "\n",
    "        noise = np.random.normal(loc=0.0, scale=noise_std_dev, size=original_feature.shape)\n",
    "        aug_feature = original_feature + noise\n",
    "        aug_label = original_label\n",
    "\n",
    "        augmented_features_list.append(aug_feature)\n",
    "        augmented_labels_list.append(aug_label)\n",
    "        augmented_basenames_list.append(f\"{original_basename}_aug{aug_count}_noise{noise_std_dev}\")\n",
    "\n",
    "        aug_count += 1\n",
    "        if aug_count % 100 == 0:\n",
    "            print(f\"Generated {aug_count} augmented samples...\")\n",
    "\n",
    "    augmented_features = np.array(augmented_features_list)\n",
    "    augmented_labels = np.array(augmented_labels_list)\n",
    "    augmented_basenames = np.array(augmented_basenames_list)\n",
    "\n",
    "    features_train_val = np.concatenate((features_train_val, augmented_features), axis=0)\n",
    "    labels_train_val = np.concatenate((labels_train_val, augmented_labels), axis=0)\n",
    "    basenames_train_val = np.concatenate((basenames_train_val, augmented_basenames), axis=0)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#################################################\n",
    "# Dataset Splitting                             #\n",
    "#################################################\n",
    "\n",
    "train_val_labels = np.array([1 if labels_train_val[i].any() else 0 for i in range(len(labels_train_val))])\n",
    "\n",
    "# 80:20 split for train-validation (on train-val set) with stratification\n",
    "train_val_size = len(features_train_val)\n",
    "indices_train, indices_val = train_test_split(\n",
    "    np.arange(train_val_size),\n",
    "    test_size=0.2,\n",
    "    shuffle=True,\n",
    "    stratify=train_val_labels\n",
    ")\n",
    "\n",
    "features_train = features_train_val[indices_train]\n",
    "labels_train = labels_train_val[indices_train]\n",
    "basenames_train = basenames_train_val[indices_train]\n",
    "\n",
    "features_val = features_train_val[indices_val]\n",
    "labels_val = labels_train_val[indices_val]\n",
    "basenames_val = basenames_train_val[indices_val]\n",
    "\n",
    "features_test = features[test_indices]\n",
    "labels_test = labels[test_indices]\n",
    "basenames_test = basenames[test_indices]\n",
    "\n",
    "n_train = len(features_train)\n",
    "n_val = len(features_val)\n",
    "n_test = len(features_test)\n",
    "\n",
    "print(f\"Train samples: {n_train}\")\n",
    "print(f\"Validation samples: {n_val}\")\n",
    "print(f\"Test samples: {n_test}\")\n",
    "\n",
    "n_dataset = n_train + n_val + n_test\n",
    "print(f\"TOTAL: {n_dataset}\")\n",
    "\n",
    "print(\"Dataset ready...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12eeb2b2-bb64-43cb-a35f-27c2ffc98e99",
   "metadata": {
    "id": "12eeb2b2-bb64-43cb-a35f-27c2ffc98e99"
   },
   "source": [
    "## Step 7: Define the Architecture  \n",
    "\n",
    "Define the structure of the convolutional neural network for shear line classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa709b18-4e1b-4ddc-b84f-2bf3d6702e6f",
   "metadata": {
    "id": "aa709b18-4e1b-4ddc-b84f-2bf3d6702e6f"
   },
   "outputs": [],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489f68ca-7e79-46e5-a966-4fd7ac38d62d",
   "metadata": {
    "id": "489f68ca-7e79-46e5-a966-4fd7ac38d62d"
   },
   "source": [
    "### Step 8: Train the Model  \n",
    "\n",
    "> Feed the training-val dataset to the compiled CNN model.  \n",
    "\n",
    "**Note:** Re-compile model again before running this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d37089-07d2-47e2-ab25-dd2553dc96c2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "07d37089-07d2-47e2-ab25-dd2553dc96c2",
    "outputId": "2ff7fbaa-7e25-4c1d-cc17-451ae90175fc"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "BATCH_SIZE = 32  # Increase to 8/16/32/64 in actual run\n",
    "ARCHITECTURE = \"UNet\"\n",
    "\n",
    "MODELS = \"models\"\n",
    "ANALYSIS = \"analysis\"\n",
    "DATASET = \"data\"\n",
    "TEST = \"test\"\n",
    "\n",
    "os.makedirs(MODELS, exist_ok=True)\n",
    "os.makedirs(ANALYSIS, exist_ok=True)\n",
    "\n",
    "training_timestamp = int(time.time())\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=10)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.8, patience=5, min_lr=1e-06)  # min α = 1e-5 - 1e-6\n",
    "history = model.fit(\n",
    "    features_train,\n",
    "    labels_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(features_val, labels_val),\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "training_duration = (int(time.time()) - training_timestamp) / 60\n",
    "print(f\"Training completed in {training_duration:.2f} minutes.\")\n",
    "\n",
    "\n",
    "fullpath = f\"{MODELS}/shearline.{ARCHITECTURE}_{training_timestamp}.keras\"\n",
    "model.save(fullpath)\n",
    "\n",
    "with open(f\"{ANALYSIS}/metrics_{training_timestamp}.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"loss\": history.history[\"loss\"],\n",
    "        \"tversky_coefficient\": history.history[\"tversky_coefficient\"],\n",
    "        \"val_loss\": history.history[\"val_loss\"],\n",
    "        \"val_tversky_coefficient\": history.history[\"val_tversky_coefficient\"],\n",
    "        \"learning_rate\": history.history[\"learning_rate\"]\n",
    "    }, f, indent=4)\n",
    "\n",
    "print(f\"Model training complete and saved to '{fullpath}'\")\n",
    "print(f\"Training and validation metrics saved to '{ANALYSIS}/metrics_{training_timestamp}.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340c1868-d11e-4eb8-a7b9-7395f672c51a",
   "metadata": {
    "id": "340c1868-d11e-4eb8-a7b9-7395f672c51a"
   },
   "source": [
    "## Step 9: Generate Training Analysis  \n",
    "\n",
    "**Metrics Definitions**\n",
    "* Loss is computed based on how far each prediction is from the ground truth, specifically using Dice Loss.\n",
    "* Dice Coefficient is a specific type of accuracy, focused on the positive class--best for imbalanced datasets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838ae30e-0e40-4505-aa94-d15ac9f3b35a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "838ae30e-0e40-4505-aa94-d15ac9f3b35a",
    "outputId": "4151532c-574f-404e-a1e7-c6cfcf1c5f6c"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "with open(f\"{ANALYSIS}/metrics_{training_timestamp}.json\", \"r\") as f:\n",
    "    metrics = json.load(f)\n",
    "\n",
    "epochs = range(1, len(metrics[\"loss\"]) + 1)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.lineplot(x=epochs, y=metrics[\"loss\"], label=\"Training Tversky Loss\", color=\"blue\")\n",
    "sns.lineplot(x=epochs, y=metrics[\"val_loss\"], label=\"Validation Tversky Loss\", color=\"orange\")\n",
    "plt.title(\"Tversky Loss vs. Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Tversky Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(f\"{ANALYSIS}/tversky_loss_plot_{training_timestamp}.png\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.lineplot(x=epochs, y=metrics[\"tversky_coefficient\"], label=\"Training Tversky Coefficient\", color=\"green\")\n",
    "sns.lineplot(x=epochs, y=metrics[\"val_v_coefficient\"], label=\"Validation Tversky Coefficient\", color=\"red\")\n",
    "plt.title(\"Tversky Coefficient vs. Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Tversky Coefficient\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(f\"{ANALYSIS}/tversky_coefficient_plot_{training_timestamp}.png\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.lineplot(x=epochs, y=metrics[\"learning_rate\"], label=\"Learning Rate\", color=\"violet\")\n",
    "plt.title(\"Learning Rate vs. Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(f\"{ANALYSIS}/learning_rate_{training_timestamp}.png\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPlots saved to {ANALYSIS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04709c7-826a-405e-aad1-8dcf59acad51",
   "metadata": {
    "id": "b04709c7-826a-405e-aad1-8dcf59acad51"
   },
   "source": [
    "## Step 10: Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3994e241-f4d0-4587-947d-9eb50bdcb7a6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3994e241-f4d0-4587-947d-9eb50bdcb7a6",
    "outputId": "82057aa7-5848-4d96-80cd-133c035cd00a"
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "results = model.predict(features_test, verbose=1)\n",
    "prediction_duration = time.time() - start_time\n",
    "image_prediction_time = prediction_duration / len(features_test)\n",
    "\n",
    "predictions = (results.squeeze(-1) > 0.5).astype(\"int32\")\n",
    "predictions_flat = predictions.flatten()\n",
    "labels_test_flat = labels_test.flatten()\n",
    "\n",
    "print(f\"Total prediction time: {prediction_duration:.4f} seconds\")\n",
    "print(f\"Time per spatial map: {image_prediction_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bd7c38-2cc2-4ea2-b4db-26f7663063ba",
   "metadata": {
    "id": "c5bd7c38-2cc2-4ea2-b4db-26f7663063ba"
   },
   "source": [
    "## Step 11: Display the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fca587a-9287-453d-b86c-1023af269c92",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8fca587a-9287-453d-b86c-1023af269c92",
    "outputId": "e4ea8e9c-33ad-4c46-b141-d2ea86968f39",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "\n",
    "def visualize_comparison(filename, true_mask, predicted_mask):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(true_mask, cmap=\"gray\")\n",
    "    plt.title(f\"True Mask: {filename}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(predicted_mask, cmap=\"gray\")\n",
    "    plt.title(f\"Predicted Mask: {filename}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "for filename, true_mask, predicted_mask in zip(basenames_test, labels_test, predictions):\n",
    "    visualize_comparison(filename, true_mask, predicted_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479984a7-a80d-49ab-96d5-e6ab8bb9e9c9",
   "metadata": {
    "id": "479984a7-a80d-49ab-96d5-e6ab8bb9e9c9"
   },
   "source": [
    "## Step 12: Pixel-wise Segmentation Evaluation\n",
    "\n",
    "> Evaluate the spatial map accuracy of predicted shear line binary mask using segmentation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb56799-76ed-45a8-96d4-5dfcd015577f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 550
    },
    "id": "5cb56799-76ed-45a8-96d4-5dfcd015577f",
    "outputId": "52486f39-395a-461b-d5ed-27fe9e6209fc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def iou(y_true, y_pred):\n",
    "    intersection = np.logical_and(y_true.astype(bool), y_pred.astype(bool)).sum()\n",
    "    union = np.logical_or(y_true.astype(bool), y_pred.astype(bool)).sum()\n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "def dice(y_true, y_pred):\n",
    "    intersection = np.logical_and(y_true.astype(bool), y_pred.astype(bool)).sum()\n",
    "    denominator = y_true.astype(bool).sum() + y_pred.astype(bool).sum()\n",
    "    return 2 * intersection / denominator if denominator > 0 else 0.0\n",
    "\n",
    "def tversky(y_true, y_pred, alpha=0.7, beta=0.3):\n",
    "    y_true_f = y_true.astype(bool)\n",
    "    y_pred_f = y_pred.astype(bool)\n",
    "    TP = np.logical_and(y_true_f, y_pred_f).sum()\n",
    "    FP = np.logical_and(np.logical_not(y_true_f), y_pred_f).sum()\n",
    "    FN = np.logical_and(y_true_f, np.logical_not(y_pred_f)).sum()\n",
    "    denominator = TP + alpha * FP + beta * FN\n",
    "    return TP / denominator if denominator > 0 else 0.0\n",
    "\n",
    "ious = []\n",
    "dices = []\n",
    "tverskys = []\n",
    "accuracies = []\n",
    "\n",
    "for t_mask, p_mask in zip(labels_test, predictions):\n",
    "    t_mask = t_mask.reshape(161, 141)\n",
    "    p_mask = p_mask.reshape(161, 141)\n",
    "\n",
    "    ious.append(iou(t_mask, p_mask))\n",
    "    dices.append(dice(t_mask, p_mask))\n",
    "    tverskys.append(tversky(t_mask, p_mask))\n",
    "    accuracies.append(np.mean(t_mask.flatten() == p_mask.flatten()))\n",
    "\n",
    "iou_score = np.mean(ious)\n",
    "dice_score = np.mean(dices)\n",
    "tversky_score = np.mean(tverskys)\n",
    "\n",
    "true_all = np.concatenate([t.flatten() for t in labels_test])\n",
    "pred_all = np.concatenate([p.flatten() for p in predictions])\n",
    "\n",
    "cm = confusion_matrix(true_all, pred_all, labels=[0, 1])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"No Shear Line\", \"Shear Line\"])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "disp.plot(cmap=\"magma\", ax=ax)\n",
    "plt.title(f\"IoU={iou_score:.4f}, Dice={dice_score:.4f}, Tversky={tversky_score:.4f}\")\n",
    "plt.savefig(f\"{ANALYSIS}/confusion_matrix_{training_timestamp}.png\")\n",
    "plt.show()\n",
    "\n",
    "interpretations = {\n",
    "    (0.90, 1.00): \"Excellent\",\n",
    "    (0.75, 0.90): \"Good\",\n",
    "    (0.50, 0.75): \"Moderate\",\n",
    "    (0.20, 0.50): \"Poor\",\n",
    "    (0.00, 0.20): \"Very Poor\"\n",
    "}\n",
    "\n",
    "for score_range, interpretation in interpretations.items():\n",
    "    if score_range[0] <= iou_score <= score_range[1]:\n",
    "        print(f\"IoU Interpretation: {interpretation}\")\n",
    "        break\n",
    "\n",
    "for score_range, interpretation in interpretations.items():\n",
    "    if score_range[0] <= dice_score <= score_range[1]:\n",
    "        print(f\"Dice Interpretation: {interpretation}\")\n",
    "        break\n",
    "\n",
    "for score_range, interpretation in interpretations.items():\n",
    "    if score_range[0] <= tversky_score <= score_range[1]:\n",
    "        print(f\"Tversky Interpretation: {interpretation}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f929d9-23ff-43c4-9d22-5363a1cf06ad",
   "metadata": {
    "id": "06f929d9-23ff-43c4-9d22-5363a1cf06ad"
   },
   "source": [
    "# Step 13: Duplicate Notebook  \n",
    "\n",
    "**Note:** Manually save first before duplicating the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e931bd5a-682c-46bc-abfe-69bd6aa3088c",
   "metadata": {
    "id": "e931bd5a-682c-46bc-abfe-69bd6aa3088c",
    "outputId": "587f7698-0033-4320-c944-de423ca106ca",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "VALIDATIONS = \"validations\"\n",
    "os.makedirs(VALIDATIONS, exist_ok=True)\n",
    "\n",
    "filename = \"ShearLineCNN.ipynb\"\n",
    "with open(filename, \"r\", encoding=\"utf-8\") as src:\n",
    "    contents = src.read()\n",
    "    checkpoint = f\"{VALIDATIONS}/{filename}\".replace(\".ipynb\", f\"_{training_timestamp}.ipynb\")\n",
    "    with open(checkpoint, \"w\", encoding=\"utf-8\") as dest:\n",
    "        dest.write(contents)\n",
    "        print(f\"Checkpoint was created at '{checkpoint}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a631ad5d-420c-4996-b772-1d6df7cac18d",
   "metadata": {
    "id": "a631ad5d-420c-4996-b772-1d6df7cac18d"
   },
   "source": [
    "> End of code."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
