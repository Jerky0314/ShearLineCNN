{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ae5d8f43-6828-4068-be52-b1695b6f0c43",
      "metadata": {
        "id": "ae5d8f43-6828-4068-be52-b1695b6f0c43"
      },
      "source": [
        "# ShearLineCNN\n",
        "> Shear Line Classification using CNN."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db17189e-3c33-4faa-8b17-2085be1342a5",
      "metadata": {
        "id": "db17189e-3c33-4faa-8b17-2085be1342a5"
      },
      "source": [
        "## Revision History\n",
        "\n",
        "| #   | Date       | Action                                           | Modified by        |\n",
        "|-----|------------|--------------------------------------------------|--------------------|\n",
        "|     |            |                                                  |                    |\n",
        "| 010 | 2025-05-16 | Optimize architecture                            | rmaniego           |\n",
        "| 009 | 2025-05-16 | Improve architecture                             | rmaniego           |\n",
        "| 008 | 2025-05-15 | Fix model metrics                                | rmaniego           |\n",
        "| 007 | 2025-05-03 | Fix testing evaluation                           | rmaniego           |\n",
        "| 006 | 2025-05-03 | Fix dataset loader                               | rmaniego           |\n",
        "| 005 | 2025-05-03 | Fix segmentation dataset                         | rmaniego           |\n",
        "| 004 | 2025-04-10 | Fix architecture to match dataset                | rmaniego           |\n",
        "| 003 | 2025-04-10 | Update architecture base codes                   | rmaniego           |\n",
        "| 002 | 2025-04-09 | Prepare dataset                                  | rmaniego           |\n",
        "| 001 | 2025-03-29 | Create GitHub repository                         | rmaniego           |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59e0c792-5a86-47d4-84e2-f77c1b7187a9",
      "metadata": {
        "id": "59e0c792-5a86-47d4-84e2-f77c1b7187a9"
      },
      "source": [
        "## Step 1. Mount Google Drive\n",
        "\n",
        "**Notes:**.\n",
        " - This requires GDrive permissions.\n",
        " - Update changes in local repository.\n",
        " - Re-run cell for every commit changes in the repository.\n",
        " - Colab is read only, unless set in GitHub FGPATs\n",
        "\n",
        "```python\n",
        "pip install jupyterlab\n",
        "pip install notebook\n",
        "jupyter notebook\n",
        "```\n",
        "\n",
        "**GitHub Personal Access Tokens (PAT)**\n",
        "1. Go to `https://github.com/settings/tokens`.\n",
        "2. On the sidebar, select `Fine-grained tokens`.\n",
        "3. Fill-up appropriate details, limit read/write access.\n",
        "4. Copy generated `PAT` to local environment variables.\n",
        "5. Do the same to Google Colab secrets.\n",
        "6. Once expired, move the old repo in GDrive to trash."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5ff34bdd-3fe1-4390-aebc-84f07d013ad9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ff34bdd-3fe1-4390-aebc-84f07d013ad9",
        "outputId": "0eed7991-3c22-4f19-ef62-ff4b0ea1c49d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Running on Google Colaboratory...\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "github_fgpat = None\n",
        "live_on_colab = False\n",
        "environment_ready = False\n",
        "\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
        "\n",
        "try:\n",
        "    from google.colab import drive, userdata\n",
        "\n",
        "    drive.mount(\"/content/drive\")\n",
        "\n",
        "    live_on_colab = True\n",
        "    github_fgpat = userdata.get(\"ShearLineCNN\")\n",
        "    print(\"Running on Google Colaboratory...\")\n",
        "except ImportError:\n",
        "    print(\"Running locally...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83cd2a48-9c27-4863-b23f-f635c33a0789",
      "metadata": {
        "id": "83cd2a48-9c27-4863-b23f-f635c33a0789"
      },
      "source": [
        "## Step 2. Check Colab Compute Engine Backend\n",
        "**Note:** Execute to verify HW accelerator allocation, use information on manuscript.\n",
        "\n",
        "HW accelerator availability may vary, so ensure that the session is timed and is connected to expected runtime environment in all iterations. Options include:\n",
        "1. NVIDIA A100 Tensor Core GPU - high-performance deep learning training (recommended).\n",
        "2. NVIDIA L4 Tensor Core GPU - optimized for AI inference tasks with high performance and efficiency (preferred during HP fine-tuning).\n",
        "3. NVIDIA T4 Tensor Core GPU - cost-effective, versatile, and suitable for a variety of tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a6b6346a-0cf0-45ae-adac-7ec8d8a08bcc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6b6346a-0cf0-45ae-adac-7ec8d8a08bcc",
        "outputId": "7f97fdc9-78ab-42c0-98dd-7e88d6b612ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon May 19 09:12:35 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0             43W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "Your runtime has 89.6 gigabytes of available RAM\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "if live_on_colab:\n",
        "    gpu_info = !nvidia-smi\n",
        "    gpu_info = \"\\n\".join(gpu_info)\n",
        "    if gpu_info.find(\"failed\") >= 0:\n",
        "        print(\"Not connected to a GPU\")\n",
        "    else:\n",
        "        print(gpu_info)\n",
        "\n",
        "    from psutil import virtual_memory\n",
        "\n",
        "    ram_gb = virtual_memory().total / 1e9\n",
        "    print(f\"Your runtime has {ram_gb:.1f} gigabytes of available RAM\")\n",
        "\n",
        "    if ram_gb < 20:\n",
        "        print(\"Not using a high-RAM runtime\")\n",
        "    else:\n",
        "        print(\"You are using a high-RAM runtime!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "846e48d4-07e6-4f8d-a469-695cc37c6354",
      "metadata": {
        "id": "846e48d4-07e6-4f8d-a469-695cc37c6354"
      },
      "source": [
        "## Step 3. Change Working Directory\n",
        "**Note:** Execute cell to ensure the notebook is running under the latest version of project repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ae29c92-27d9-4154-8951-328298daa982",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ae29c92-27d9-4154-8951-328298daa982",
        "outputId": "76af8fe7-7b8e-4247-b91e-05e450b237ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ],
      "source": [
        "if live_on_colab:\n",
        "    NB = \"/content/drive/MyDrive/Colab Notebooks/\"\n",
        "    os.makedirs(NB, exist_ok=True)\n",
        "\n",
        "    def update_repo():\n",
        "\n",
        "        REPO = f\"{NB}/ShearLineCNN\"\n",
        "        if not os.path.isdir(REPO):\n",
        "            !git clone https://{github_fgpat}@github.com/Jerky0314/ShearLineCNN.git\n",
        "            os.chdir(NB)\n",
        "            return\n",
        "\n",
        "        os.chdir(REPO)\n",
        "        !git reset --hard HEAD\n",
        "        !git pull origin main\n",
        "\n",
        "    update_repo()\n",
        "\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RwG-WN4bYPzN",
      "metadata": {
        "id": "RwG-WN4bYPzN"
      },
      "outputs": [],
      "source": [
        " !git config pull.rebase false"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a6a8531-7976-4e6f-b943-4a72e056ed1d",
      "metadata": {
        "id": "1a6a8531-7976-4e6f-b943-4a72e056ed1d"
      },
      "source": [
        "## Step 4. Install Dependencies\n",
        "**Note:** Execute cell everytime the `Google Colab` runtime environment reconnected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dafb19c-b37d-4527-8f64-ab4e12efd6c8",
      "metadata": {
        "id": "7dafb19c-b37d-4527-8f64-ab4e12efd6c8"
      },
      "outputs": [],
      "source": [
        "if live_on_colab:\n",
        "    %pip install -U jupyterlab\n",
        "    %pip install -U notebook\n",
        "    %pip install -U opencv-python\n",
        "    %pip install -U scikit-learn\n",
        "    %pip install -U scikit-image\n",
        "    %pip install -U matplotlib\n",
        "    %pip install -U seaborn\n",
        "    %pip install -U tensorflow\n",
        "    %pip install -U tabulate\n",
        "print(\"Environment is ready...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e09b829-9e4e-405e-ba4c-cf01e76cde4b",
      "metadata": {
        "id": "0e09b829-9e4e-405e-ba4c-cf01e76cde4b"
      },
      "source": [
        "## Step 5: Import the Packages  \n",
        "\n",
        "import all third party libraries necessary for the ANN model to execute successfully."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73d9d045-7f6b-43af-9271-25872b31a013",
      "metadata": {
        "id": "73d9d045-7f6b-43af-9271-25872b31a013"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import json\n",
        "import time\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\"os.fork()\")\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, BatchNormalization, Dropout, UpSampling2D, Cropping2D\n",
        "from tensorflow.keras.optimizers import Adadelta, Lion, SGD\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "\n",
        "gpus = tf.config.list_physical_devices(\"GPU\")\n",
        "\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    except RuntimeError as e:\n",
        "            print(e)\n",
        "    print(\"GPU detected. Running on GPU.\")\n",
        "else:\n",
        "    print(\"No GPU detected. Running on CPU.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb73f5f4-ab60-487e-96a0-45c562c5688b",
      "metadata": {
        "id": "bb73f5f4-ab60-487e-96a0-45c562c5688b"
      },
      "source": [
        "## Step 6: Load Datasets  \n",
        "\n",
        "Load and prepare the training and testing datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2167f830-dd47-48bf-a7e5-496c1227cebf",
      "metadata": {
        "id": "2167f830-dd47-48bf-a7e5-496c1227cebf"
      },
      "outputs": [],
      "source": [
        "def load_features(source, target, category):\n",
        "    basenames, features, labels = [], [], []\n",
        "\n",
        "    sources = glob.glob(f\"{source}/{category}/*.json\")\n",
        "    for i, source_path in enumerate(sources):\n",
        "        filename = os.path.basename(source_path)\n",
        "\n",
        "        with open(source_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            data1 = np.array(json.load(file))\n",
        "\n",
        "        target_path = f\"{target}/{category}/{filename}\"\n",
        "        with open(target_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            data2 = np.array(json.load(file))\n",
        "\n",
        "        features.append(data1)\n",
        "        labels.append(data2)\n",
        "        basenames.append(filename)\n",
        "\n",
        "    return basenames, np.array(features), np.array(labels)\n",
        "\n",
        "basenames_list, features_list, labels_list = [], [], []\n",
        "\n",
        "categories = [\"no-shear\", \"shear\"]\n",
        "segmentation_source = \"data/segmentation/source\"\n",
        "segmentation_target = \"data/segmentation/target\"\n",
        "for category in categories:\n",
        "    basenames, features, labels = load_features(segmentation_source, segmentation_target, category)\n",
        "    if features.size > 0:\n",
        "        basenames_list.extend(basenames)\n",
        "        features_list.append(features)\n",
        "        labels_list.append(labels)\n",
        "\n",
        "features = np.vstack(features_list)\n",
        "labels = np.vstack(labels_list)\n",
        "basenames = np.array(basenames_list)\n",
        "\n",
        "# 90:10 split for testing (only split basenames)\n",
        "basenames_train_val, basenames_test = train_test_split(\n",
        "    basenames, test_size=0.1, shuffle=True\n",
        ")\n",
        "\n",
        "mask_train_val = [name in basenames_train_val for name in basenames]\n",
        "features_train_val = features[mask_train_val]\n",
        "labels_train_val = labels[mask_train_val]\n",
        "\n",
        "# split features and labels based on the new splits\n",
        "mask_test = [name in basenames_test for name in basenames]\n",
        "features_test = features[mask_test]\n",
        "labels_test = labels[mask_test]\n",
        "\n",
        "# 70:30 split for train-validation\n",
        "indices_train, indices_val = train_test_split(\n",
        "    np.arange(len(basenames_train_val)), test_size=0.4, shuffle=True\n",
        ")\n",
        "\n",
        "features_train = features_train_val[indices_train]\n",
        "labels_train = labels_train_val[indices_train]\n",
        "basenames_train = basenames_train_val[indices_train]\n",
        "\n",
        "features_val = features_train_val[indices_val]\n",
        "labels_val = labels_train_val[indices_val]\n",
        "basenames_val = basenames_train_val[indices_val]\n",
        "\n",
        "print(\"Dataset ready...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12eeb2b2-bb64-43cb-a35f-27c2ffc98e99",
      "metadata": {
        "id": "12eeb2b2-bb64-43cb-a35f-27c2ffc98e99"
      },
      "source": [
        "## Step 7: Define the Architecture  \n",
        "\n",
        "Define the structure of the convolutional neural network for shear line classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa709b18-4e1b-4ddc-b84f-2bf3d6702e6f",
      "metadata": {
        "id": "aa709b18-4e1b-4ddc-b84f-2bf3d6702e6f"
      },
      "outputs": [],
      "source": [
        "def dice_coefficient(y_true, y_pred, smooth=1e-6):\n",
        "    \"\"\"Dice coefficient for binary segmentation.\"\"\"\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(K.cast(y_true_f, \"float32\") * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (K.sum(K.cast(y_true_f, \"float32\")) + K.sum(K.cast(y_pred_f, \"float32\")) + smooth)\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    \"\"\"Dice loss (1 - Dice coefficient).\"\"\"\n",
        "    return 1 - dice_coefficient(y_true, y_pred)\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(161, 141, 2)))\n",
        "\n",
        "# Encoders\n",
        "#model.add(Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\"))\n",
        "3#model.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\"))\n",
        "# model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
        "# model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "# Bottleneck\n",
        "model.add(Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\"))\n",
        "# model.add(Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\"))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Decoders\n",
        "model.add(UpSampling2D(size=(2, 2)))\n",
        "model.add(Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\"))\n",
        "# model.add(BatchNormalization())\n",
        "\n",
        "model.add(UpSampling2D(size=(2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\"))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# model.add(UpSampling2D(size=(2, 2)))\n",
        "# model.add(Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\"))\n",
        "# model.add(BatchNormalization())\n",
        "\n",
        "model.add(Cropping2D(cropping=((2, 1), (2, 1))))\n",
        "model.add(Conv2D(1, (1, 1), activation=\"sigmoid\"))\n",
        "\n",
        "# Compile Model\n",
        "# optimizer = Adadelta(learning_rate=1.0, rho=0.95)\n",
        "# optimizer = Lion(learning_rate=0.001)\n",
        "optimizer = SGD(learning_rate=0.01)\n",
        "model.compile(optimizer=optimizer, loss=dice_loss, metrics=[dice_coefficient])\n",
        "model.summary()\n",
        "\n",
        "# Notes: 2025-05-15\n",
        "# The issue here is that BCE is not reliable when there is a class imbalance.\n",
        "# The class pointed here is the 0s and 1s, where 0s dominates 1s.\n",
        "# If you study the data, the annotation of the shearline only draws a very thin line.\n",
        "# This clearly shows that there are more blacks than whites, in simple terms.\n",
        "# So, the model can become more biased to generalize (or predict) more blacks than whites.\n",
        "# This can cause inaccuracies by predicting blacks even if should be a white.\n",
        "# An option is use Focal Loss by which adds more weight to whites (1s) than blacks (0s)\n",
        "# to compensate the imbalance. This requires repeated trials to find the optimal weight.\n",
        "# Simply, focal loss is a strategy to force learning on the under represented class.\n",
        "# Another option is to use DICE loss... It is a sort of another \"accuracy\" metric.\n",
        "# Unlike the generic \"accuarcy\" metric, it only computed on the positive (1s) class,\n",
        "# penalizes FN and FP strongly (ignores TN). Formula below is just for analogy:\n",
        "#  * Accuracy = (TP + TN) / (TP + TN + FP + FN) ---> works well with balanced classes.\n",
        "#  * Dice = 2TP / (2TP + FP + FN)               ---> best with imbalanced classes.\n",
        "# On multiple tests, Focal Loss did not perform well, but Dice shown potential for further tests.\n",
        "# The accuracy metric was removed, since it was irrelevant in this study.\n",
        "# Metrics that will be used Dice Loss and Dice Coefficient"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "489f68ca-7e79-46e5-a966-4fd7ac38d62d",
      "metadata": {
        "id": "489f68ca-7e79-46e5-a966-4fd7ac38d62d"
      },
      "source": [
        "### Step 8: Train the Model  \n",
        "\n",
        "Feed the training-val dataset to the compiled CNN model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07d37089-07d2-47e2-ab25-dd2553dc96c2",
      "metadata": {
        "id": "07d37089-07d2-47e2-ab25-dd2553dc96c2"
      },
      "outputs": [],
      "source": [
        "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"warn\")\n",
        "\n",
        "EPOCHS = 500    # Increase to 100/200/500/1000 epochs in actual run\n",
        "BATCH_SIZE = 4  # Increase to 8/16/32 epochs in actual run\n",
        "ARCHITECTURE = \"CNN\"\n",
        "\n",
        "MODELS = \"models\"\n",
        "ANALYSIS = \"analysis\"\n",
        "DATASET = \"data\"\n",
        "TEST = \"test\"\n",
        "\n",
        "os.makedirs(MODELS, exist_ok=True)\n",
        "os.makedirs(ANALYSIS, exist_ok=True)\n",
        "\n",
        "training_timestamp = int(time.time())\n",
        "\n",
        "history = model.fit(\n",
        "    features_train,\n",
        "    labels_train,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_data=(features_val, labels_val)\n",
        ")\n",
        "\n",
        "training_duration = (int(time.time()) - training_timestamp) / 60\n",
        "\n",
        "print(f\"Training completed in {training_duration:.2f} minutes.\")\n",
        "\n",
        "fullpath = f\"{MODELS}/shearline.{ARCHITECTURE}_{training_timestamp}.keras\"\n",
        "model.save(fullpath)\n",
        "\n",
        "with open(f\"{ANALYSIS}/metrics_{training_timestamp}.json\", \"w\") as f:\n",
        "    json.dump({\n",
        "        \"loss\": history.history[\"loss\"],\n",
        "        \"dice_coefficient\": history.history[\"dice_coefficient\"],\n",
        "        \"val_loss\": history.history[\"val_loss\"],\n",
        "        \"val_dice_coefficient\": history.history[\"val_dice_coefficient\"]\n",
        "    }, f, indent=4)\n",
        "\n",
        "print(f\"Model training complete and saved to '{fullpath}'\")\n",
        "print(f\"Training and validation metrics saved to '{ANALYSIS}/metrics_{training_timestamp}.json'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "340c1868-d11e-4eb8-a7b9-7395f672c51a",
      "metadata": {
        "id": "340c1868-d11e-4eb8-a7b9-7395f672c51a"
      },
      "source": [
        "## Step 9: Generate Training Analysis  \n",
        "\n",
        "**Metrics Definitions**\n",
        "* Loss is computed based on how far each prediction is from the ground truth, specifically using Dice Loss.\n",
        "* Dice Coefficient is a specific type of accuracy, focused on the positive class--best for imbalanced datasets.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "838ae30e-0e40-4505-aa94-d15ac9f3b35a",
      "metadata": {
        "id": "838ae30e-0e40-4505-aa94-d15ac9f3b35a"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "with open(f\"{ANALYSIS}/metrics_{training_timestamp}.json\", \"r\") as f:\n",
        "    metrics = json.load(f)\n",
        "\n",
        "epochs = range(1, len(metrics[\"loss\"]) + 1)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.lineplot(x=epochs, y=metrics[\"loss\"], label=\"Training Dice Loss\", color=\"blue\")\n",
        "sns.lineplot(x=epochs, y=metrics[\"val_loss\"], label=\"Validation Dice Loss\", color=\"orange\")\n",
        "plt.title(\"Dice Loss vs. Epochs\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Dice Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig(f\"{ANALYSIS}/dice_loss_plot_{training_timestamp}.png\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.lineplot(x=epochs, y=metrics[\"dice_coefficient\"], label=\"Training Dice Coefficient\", color=\"green\")\n",
        "sns.lineplot(x=epochs, y=metrics[\"val_dice_coefficient\"], label=\"Validation Dice Coefficient\", color=\"red\")\n",
        "plt.title(\"Dice Coefficient vs. Epochs\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Dice Coefficient\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig(f\"{ANALYSIS}/dice_coefficient_plot_{training_timestamp}.png\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nPlots saved to {ANALYSIS}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b04709c7-826a-405e-aad1-8dcf59acad51",
      "metadata": {
        "id": "b04709c7-826a-405e-aad1-8dcf59acad51"
      },
      "source": [
        "## Step 10: Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3994e241-f4d0-4587-947d-9eb50bdcb7a6",
      "metadata": {
        "id": "3994e241-f4d0-4587-947d-9eb50bdcb7a6"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "results = model.predict(features_test, verbose=1)\n",
        "prediction_duration = time.time() - start_time\n",
        "image_prediction_time = prediction_duration / len(features_test)\n",
        "\n",
        "predictions = (results.squeeze(-1) > 0.5).astype(\"int32\")\n",
        "predictions_flat = predictions.flatten()\n",
        "labels_test_flat = labels_test.flatten()\n",
        "\n",
        "print(f\"Total prediction time: {prediction_duration:.4f} seconds\")\n",
        "print(f\"Time per image: {image_prediction_time:.4f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5bd7c38-2cc2-4ea2-b4db-26f7663063ba",
      "metadata": {
        "id": "c5bd7c38-2cc2-4ea2-b4db-26f7663063ba"
      },
      "source": [
        "## Step 11: Display the Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fca587a-9287-453d-b86c-1023af269c92",
      "metadata": {
        "id": "8fca587a-9287-453d-b86c-1023af269c92",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tabulate import tabulate\n",
        "\n",
        "def visualize_comparison(filename, true_mask, predicted_mask):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(true_mask, cmap=\"gray\")\n",
        "    plt.title(f\"True Mask: {filename}\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(predicted_mask, cmap=\"gray\")\n",
        "    plt.title(f\"Predicted Mask: {filename}\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "for filename, true_mask, predicted_mask in zip(basenames_test, labels_test, predictions):\n",
        "    visualize_comparison(filename, true_mask, predicted_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "479984a7-a80d-49ab-96d5-e6ab8bb9e9c9",
      "metadata": {
        "id": "479984a7-a80d-49ab-96d5-e6ab8bb9e9c9"
      },
      "source": [
        "## Step 12: Pixel-wise Segmentation Evaluation\n",
        "\n",
        "> Evaluate the spatial map accuracy of predicted shear line binary mask using segmentation metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cb56799-76ed-45a8-96d4-5dfcd015577f",
      "metadata": {
        "id": "5cb56799-76ed-45a8-96d4-5dfcd015577f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "def iou(y_true, y_pred):\n",
        "    intersection = np.logical_and(y_true.astype(bool), y_pred.astype(bool)).sum()\n",
        "    union = np.logical_or(y_true.astype(bool), y_pred.astype(bool)).sum()\n",
        "    return intersection / union if union > 0 else 0.0\n",
        "\n",
        "def dice(y_true, y_pred):\n",
        "    intersection = np.logical_and(y_true.astype(bool), y_pred.astype(bool)).sum()\n",
        "    denominator = y_true.astype(bool).sum() + y_pred.astype(bool).sum()\n",
        "    return 2 * intersection / denominator if denominator > 0 else 0.0\n",
        "\n",
        "ious = []\n",
        "dices = []\n",
        "accuracies = []\n",
        "\n",
        "for t_mask, p_mask in zip(labels_test, predictions):\n",
        "    t_mask = t_mask.reshape(161, 141)\n",
        "    p_mask = p_mask.reshape(161, 141)\n",
        "\n",
        "    ious.append(iou(t_mask, p_mask))\n",
        "    dices.append(dice(t_mask, p_mask))\n",
        "    accuracies.append(np.mean(t_mask.flatten() == p_mask.flatten()))\n",
        "\n",
        "iou_score = np.mean(ious)\n",
        "dice_score = np.mean(dices)\n",
        "pixel_accuracy = np.mean(accuracies)\n",
        "\n",
        "true_all = np.concatenate([t.flatten() for t in labels_test])\n",
        "pred_all = np.concatenate([p.flatten() for p in predictions])\n",
        "\n",
        "cm = confusion_matrix(true_all, pred_all, labels=[0, 1])\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"No Shear Line\", \"Shear Line\"])\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "disp.plot(cmap=\"magma\", ax=ax)\n",
        "plt.title(f\"IoU={iou_score:.4f}, Dice={dice_score:.4f}\")\n",
        "plt.savefig(f\"{ANALYSIS}/confusion_matrix_{training_timestamp}.png\")\n",
        "plt.show()\n",
        "\n",
        "iou_interpretation = {\n",
        "    (0.90, 1.00): \"Excellent\",\n",
        "    (0.75, 0.90): \"Good\",\n",
        "    (0.50, 0.75): \"Moderate\",\n",
        "    (0.20, 0.50): \"Poor\",\n",
        "    (0.00, 0.20): \"Very Poor\"\n",
        "}\n",
        "\n",
        "dice_interpretation = {\n",
        "    (0.90, 1.00): \"Excellent\",\n",
        "    (0.75, 0.90): \"Good\",\n",
        "    (0.50, 0.75): \"Moderate\",\n",
        "    (0.20, 0.50): \"Poor\",\n",
        "    (0.00, 0.20): \"Very Poor\"\n",
        "}\n",
        "\n",
        "for score_range, interpretation in iou_interpretation.items():\n",
        "    if score_range[0] <= iou_score <= score_range[1]:\n",
        "        print(f\"IoU Interpretation: {interpretation}\")\n",
        "        break\n",
        "\n",
        "for score_range, interpretation in dice_interpretation.items():\n",
        "    if score_range[0] <= dice_score <= score_range[1]:\n",
        "        print(f\"Dice Interpretation: {interpretation}\")\n",
        "        break\n",
        "\n",
        "# Notes: 2025-05-15\n",
        "# The issue with the initial codes, is that the IoU and Dice scores\n",
        "# were computed on the last prediction only. This was fixed\n",
        "# by looping on all predictions and computing each scores\n",
        "# before getting the average of all results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06f929d9-23ff-43c4-9d22-5363a1cf06ad",
      "metadata": {
        "id": "06f929d9-23ff-43c4-9d22-5363a1cf06ad"
      },
      "source": [
        "# Step 13: Duplicate Notebook  \n",
        "\n",
        "**Note:** Manually save first before duplicating the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e931bd5a-682c-46bc-abfe-69bd6aa3088c",
      "metadata": {
        "id": "e931bd5a-682c-46bc-abfe-69bd6aa3088c",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "VALIDATIONS = \"validations\"\n",
        "os.makedirs(VALIDATIONS, exist_ok=True)\n",
        "\n",
        "filename = \"ShearLineCNN.ipynb\"\n",
        "with open(filename, \"r\", encoding=\"utf-8\") as src:\n",
        "    contents = src.read()\n",
        "    checkpoint = f\"{VALIDATIONS}/{filename}\".replace(\".ipynb\", f\"_{training_timestamp}.ipynb\")\n",
        "    with open(checkpoint, \"w\", encoding=\"utf-8\") as dest:\n",
        "        dest.write(contents)\n",
        "        print(f\"Checkpoint was created at '{checkpoint}'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a631ad5d-420c-4996-b772-1d6df7cac18d",
      "metadata": {
        "id": "a631ad5d-420c-4996-b772-1d6df7cac18d"
      },
      "source": [
        "> End of code."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}